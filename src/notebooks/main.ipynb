{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81e2962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from langchain.text_splitter import (\n",
    "    RecursiveCharacterTextSplitter,\n",
    "    CharacterTextSplitter,\n",
    ")\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "\n",
    "async def create_vector_db_from_file(file_path=\"test.pdf\"):\n",
    "\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    documents = await asyncio.to_thread(loader.load)\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200,\n",
    "        add_start_index=True,\n",
    "    )\n",
    "    texts = await asyncio.to_thread(text_splitter.split_documents, documents)\n",
    "    print(len(texts))\n",
    "    embedding = HuggingFaceEmbeddings(\n",
    "        model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    )\n",
    "\n",
    "    db = await asyncio.to_thread(FAISS.from_documents, texts, embedding)\n",
    "    print(db.as_retriever)\n",
    "await create_vector_db_from_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9471ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chương 4: Mảng, Chuỗi và con trỏ \n",
      " \n",
      "Phần 1: Lý thuyết: \n",
      "1. Lớp (Class) và Đối Tượng (Object) \n",
      "Class: Bản thiết kế của đối tượng, chứa thuộc tính (data) và phương \n",
      "thức (functions). \n",
      "Object: Thể hiện cụ thể của lớp. \n",
      "Ví dụ: \n",
      "class Student {private: \n",
      "  string name;     Thuộc tính private \n",
      "  int age;  public: \n",
      "   Constructor: Khởi tạo đối tượng \n",
      "  Student(string n, int a) { \n",
      "    name = n; \n",
      "    age = a; \n",
      "  } \n",
      " \n",
      "   Phương thức public \n",
      "  void display() { \n",
      "    cout << \"Ten: \" << name << \", Tuoi: \" << age << endl; \n",
      "  }}; \n",
      " Tạo đối tượng \n",
      "Student s1(\"Alice\", 20); \n",
      "s1.display();   Output: Ten: Alice, Tuoi: 20 \n",
      "Constructor/Destructor: \n",
      "Constructor: Khởi tạo giá trị mặc định khi đối tượng được tạo. \n",
      "Destructor: Hủy đối tượng, giải phóng bộ nhớ (kí hiệu ~). \n",
      "~Student() { \n",
      "  cout << \"Doi tuong da bi huy\";} \n",
      " \n",
      "2. Tính Kế Thừa (Inheritance) \n",
      "Kế thừa: Lớp con (derived class) kế thừa thuộc tính/phương thức từ lớp cha \n",
      "(base class). \n",
      "Ví dụ: \n",
      "class Person {protected: \n",
      "  string name;public: \n",
      "  void setName(string n) { name = n; }}; \n",
      " Lớp Student kế thừa từ Personclass Student : public Person {private: \n",
      "  int studentId;public: \n",
      "  void setStudentId(int id) { studentId = id; } \n",
      "  void display() { \n",
      "    cout << \"Ten: \" << name << \", ID: \" << studentId << endl; \n",
      "  }}; \n",
      " \n",
      "3. Tính Đa Hình (Polymorphism) \n",
      "Virtual Function: Cho phép ghi đè (override) phương thức ở lớp con. \n",
      "class Animal {public: \n",
      "  virtual void sound() {   Hàm ảo \n",
      "    cout << \"Tieng keu\" << endl; \n",
      "  }}; \n",
      "class Dog : public Animal {public: \n",
      "  void sound() override {   Ghi đè \n",
      "    cout << \"Gau gau!\" << endl; \n",
      "  }}; \n",
      "int main() { \n",
      "  Animal *animal = new Dog(); \n",
      "  animal->sound();   Output: Gau gau! (Đa hình runtime) \n",
      "  delete animal; \n",
      "  return 0;} \n",
      " \n",
      "4. Thư Viện STL (Standard Template Library) \n",
      "vector: Mảng động, tự quản lý bộ nhớ. \n",
      "#include <vector> \n",
      "vector<int> numbers = {3, 1, 4}; \n",
      "numbers.push_back(5);   Thêm phần tử \n",
      "cout << numbers[2];     Output: 4 \n",
      "algorithm: Cung cấp hàm tiện ích như sort(), find(). \n",
      "#include <algorithm>sort(numbers.begin(), numbers.end());   Sắp xếp tăng dần \n",
      " \n",
      "Phần 2: Thực hành \n",
      " \n",
      "Bài 1: Xây Dựng Lớp Student Quản Lý Điểm \n",
      "Yêu cầu: \n",
      "Thuộc tính: studentId, name, grades (vector<float>). \n",
      "Phương thức: Thêm điểm, tính điểm trung bình. \n",
      "Code mẫu: \n",
      "class Student {private: \n",
      "  string name; \n",
      "  string id; \n",
      "  vector<float> grades;public: \n",
      "  Student(string n, string i) : name(n), id(i) {} \n",
      " \n",
      "  void addGrade(float grade) { \n",
      "    grades.push_back(grade); \n",
      "  } \n",
      " \n",
      "  float calculateAverage() { \n",
      "    float sum = 0; \n",
      "    for (float g : grades) sum += g; \n",
      "    return sum / grades.size(); \n",
      "  } \n",
      " \n",
      "  void displayInfo() { \n",
      "    cout << \"ID: \" << id << \", Ten: \" << name  \n",
      "         << \", Diem TB: \" << calculateAverage() << endl; \n",
      "  }}; \n",
      "int main() { \n",
      "  Student s(\"Alice\", \"SV001\"); \n",
      "  s.addGrade(8.5); \n",
      "  s.addGrade(7.0); \n",
      "  s.displayInfo();   Output: ID: SV001, Ten: Alice, Diem TB: 7.75 \n",
      "  return 0;} \n",
      " \n",
      "Bài 2: Sắp Xếp Danh Sách Sinh Viên Bằng STL \n",
      "Yêu cầu: Sắp xếp danh sách sinh viên theo điểm trung bình giảm dần. \n",
      "Code mẫu: \n",
      "bool compareStudents(const Student &a, const Student &b) { \n",
      "  return a.calculateAverage() > b.calculateAverage();} \n",
      "int main() { \n",
      "  vector<Student> students; \n",
      "  students.push_back(Student(\"Bob\", \"SV002\")); \n",
      "  students.push_back(Student(\"Alice\", \"SV001\")); \n",
      " \n",
      "   Thêm điểm cho từng sinh viên \n",
      "  students[0].addGrade(9.0); \n",
      "  students[1].addGrade(8.5); \n",
      " \n",
      "   Sắp xếp bằng hàm compareStudents \n",
      "  sort(students.begin(), students.end(), compareStudents); \n",
      " \n",
      "   In danh sách đã sắp xếp \n",
      "  for (Student &s : students) { \n",
      "    s.displayInfo(); \n",
      "  } \n",
      "  return 0;} \n",
      " \n",
      " \n",
      "Bài 3: Ứng Dụng STL Khác \n",
      "Tìm kiếm phần tử trong vector: \n",
      "vector<int> nums = {5, 3, 7, 1};auto it = find(nums.begin(), nums.end(), 7);if \n",
      "(it != nums.end()) { \n",
      "  cout << \"Tim thay tai vi tri: \" << it - nums.begin();   Output: 2} \n",
      " \n",
      "Lỗi Thường Gặp & Cách Khắc Phục \n",
      "Quên public khi kế thừa: \n",
      "class Student : Person { ... };   Mặc định là private → Lỗi!class Student : public \n",
      "Person { ... };   Đúng \n",
      "Không khởi tạo vector: Truy cập phần tử khi vector rỗng gây lỗi. \n",
      "Sai phạm vi truy cập: Truy cập thuộc tính private từ bên ngoài lớp. \n",
      " \n",
      "Tips Tối Ưu Hoá \n",
      "Dùng STL thay tự implement: Tiết kiệm thời gian (ví dụ: vector thay mảng \n",
      "động). \n",
      "Virtual destructor: Luôn khai báo destructor ảo trong lớp cơ sở nếu có đa hình. \n",
      "Range-based for loop: Duyệt STL container dễ dàng. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import fitz\n",
    "\n",
    "def extract_text_from_pdf(data_path=\"test.pdf\") -> str:\n",
    "\n",
    "    with fitz.open(data_path) as doc:\n",
    "        text = \"\".join(page.get_text() for page in doc)\n",
    "\n",
    "    return text\n",
    "\n",
    "print(extract_text_from_pdf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d077e6f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116cac15594545aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-27T08:40:26.501241Z",
     "start_time": "2025-06-27T08:40:26.484990Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'llm_output': 'This is the generated output.'}\n"
     ]
    }
   ],
   "source": [
    "from typing import Literal, TypedDict\n",
    "import uuid\n",
    "\n",
    "from langgraph.constants import START, END\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.types import interrupt, Command\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# Define the shared graph state\n",
    "class State(TypedDict):\n",
    "    llm_output: str\n",
    "    decision: str\n",
    "\n",
    "# Simulate an LLM output node\n",
    "def generate_llm_output(state: State) -> State:\n",
    "    return {\"llm_output\": \"This is the generated output.\"}\n",
    "\n",
    "# Human approval node\n",
    "def human_approval(state: State) -> Command[Literal[\"approved_path\", \"rejected_path\"]]:\n",
    "    decision = interrupt({\n",
    "        \"question\": \"Do you approve the following output?\",\n",
    "        \"llm_output\": state[\"llm_output\"]\n",
    "    })\n",
    "\n",
    "    if decision == \"approve\":\n",
    "        return Command(goto=\"approved_path\", update={\"decision\": \"approved\"})\n",
    "    else:\n",
    "        return Command(goto=\"rejected_path\", update={\"decision\": \"rejected\"})\n",
    "\n",
    "# Next steps after approval\n",
    "def approved_node(state: State) -> State:\n",
    "    print(\"✅ Approved path taken.\")\n",
    "    return state\n",
    "\n",
    "# Alternative path after rejection\n",
    "def rejected_node(state: State) -> State:\n",
    "    print(\"❌ Rejected path taken.\")\n",
    "    return state\n",
    "\n",
    "# Build the graph\n",
    "builder = StateGraph(State)\n",
    "builder.add_node(\"generate_llm_output\", generate_llm_output)\n",
    "builder.add_node(\"human_approval\", human_approval)\n",
    "builder.add_node(\"approved_path\", approved_node)\n",
    "builder.add_node(\"rejected_path\", rejected_node)\n",
    "\n",
    "builder.set_entry_point(\"generate_llm_output\")\n",
    "builder.add_edge(\"generate_llm_output\", \"human_approval\")\n",
    "builder.add_edge(\"approved_path\", END)\n",
    "builder.add_edge(\"rejected_path\", END)\n",
    "\n",
    "checkpointer = MemorySaver()\n",
    "graph = builder.compile(checkpointer=checkpointer)\n",
    "\n",
    "# Run until interrupt\n",
    "config = {\"configurable\": {\"thread_id\": uuid.uuid4()}}\n",
    "result = graph.invoke({}, config=config)\n",
    "print(result[\"__interrupt__\"])\n",
    "# Output:\n",
    "# Interrupt(value={'question': 'Do you approve the following output?', 'llm_output': 'This is the generated output.'}, ...)\n",
    "\n",
    "# Simulate resuming with human input\n",
    "# To test rejection, replace resume=\"approve\" with resume=\"reject\"\n",
    "final_result = graph.invoke(Command(resume=\"approve\"), config=config, interrupt_before=[\"human_approval\"])\n",
    "print(final_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a11b52ddfdf60c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-27T09:42:07.398847Z",
     "start_time": "2025-06-27T09:40:56.493679Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "from typing import Any, Coroutine\n",
    "\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph_swarm import create_swarm\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import StateGraph\n",
    "from src.agents.manage.manage import agent as agent1\n",
    "from src.agents.chat.chat import agent as agent2\n",
    "from src.agents.state import State\n",
    "from src.config.setup import GOOGLE_API_KEY\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langgraph.types import interrupt, Command\n",
    "\n",
    "model = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    google_api_key=GOOGLE_API_KEY,\n",
    "    disable_streaming=False,\n",
    ")\n",
    "\n",
    "\n",
    "async def supervisor(state: State):\n",
    "    resp = await model.ainvoke(state[\"messages\"] + [SystemMessage(\n",
    "        content=\"\"\"You are an expert router, reading the user content and returning a word: \"chat\" if it's a normal chat request, or \"manage\" if it's a task management request like checking orders, processing requests from the system.\"\"\")])\n",
    "    print(resp)\n",
    "    resp = resp.content.strip()\n",
    "\n",
    "    if \"chat\" == resp:\n",
    "        return Command(goto=\"chat\",\n",
    "                       update={\"active_agent\": \"chat\"}, )\n",
    "    elif \"manage\" == resp:\n",
    "        return Command(goto=\"manage\",\n",
    "                       update={\"active_agent\": \"manage\"}, )\n",
    "\n",
    "    return Command(goto=\"chat\", update={\"active_agent\": \"chat\"})\n",
    "\n",
    "\n",
    "checkpointer = MemorySaver()\n",
    "\n",
    "workflow = StateGraph(State)\n",
    "workflow.add_node(\"chat\", agent1)\n",
    "workflow.add_node(\"manage\", agent2)\n",
    "workflow.add_node(\"supervisor\", supervisor)\n",
    "workflow.set_entry_point(\"supervisor\")\n",
    "\n",
    "swarm = workflow.compile(checkpointer=checkpointer)\n",
    "\n",
    "display(Image(swarm.get_graph().draw_mermaid_png()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83caa1652e4cd0c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-27T08:59:34.342469Z",
     "start_time": "2025-06-27T08:59:34.316900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entered `parent_node` a total of 1 times\n",
      "Entered `node_in_subgraph` a total of 1 times\n",
      "Entered human_node in sub-graph a total of 1 times\n",
      "{'__interrupt__': (Interrupt(value='what is your name?', resumable=True, ns=['parent_node:c82e3fef-1cd5-e3c6-aad7-dba5000d3fa4', 'human_node:1cd4b043-1782-3e7d-6d89-32a36e61b9d6']),)}\n",
      "--- Resuming ---\n",
      "Entered `parent_node` a total of 2 times\n",
      "Entered human_node in sub-graph a total of 2 times\n",
      "Got an answer of Lưu Trọng Dũng\n",
      "{'parent_node': {'state_counter': 1}}\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "from typing import TypedDict\n",
    "\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.constants import START\n",
    "from langgraph.types import interrupt, Command\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    \"\"\"The graph state.\"\"\"\n",
    "    state_counter: int\n",
    "\n",
    "\n",
    "counter_node_in_subgraph = 0\n",
    "\n",
    "def node_in_subgraph(state: State):\n",
    "    \"\"\"A node in the sub-graph.\"\"\"\n",
    "    global counter_node_in_subgraph\n",
    "    counter_node_in_subgraph += 1  # This code will **NOT** run again!\n",
    "    print(f\"Entered `node_in_subgraph` a total of {counter_node_in_subgraph} times\")\n",
    "\n",
    "counter_human_node = 0\n",
    "\n",
    "def human_node(state: State):\n",
    "    global counter_human_node\n",
    "    counter_human_node += 1 # This code will run again!\n",
    "    print(f\"Entered human_node in sub-graph a total of {counter_human_node} times\")\n",
    "    answer = interrupt(\"what is your name?\")\n",
    "    print(f\"Got an answer of {answer}\")\n",
    "\n",
    "\n",
    "checkpointer = MemorySaver()\n",
    "\n",
    "subgraph_builder = StateGraph(State)\n",
    "subgraph_builder.add_node(\"some_node\", node_in_subgraph)\n",
    "subgraph_builder.add_node(\"human_node\", human_node)\n",
    "subgraph_builder.add_edge(START, \"some_node\")\n",
    "subgraph_builder.add_edge(\"some_node\", \"human_node\")\n",
    "subgraph = subgraph_builder.compile(checkpointer=checkpointer)\n",
    "\n",
    "\n",
    "counter_parent_node = 0\n",
    "\n",
    "def parent_node(state: State):\n",
    "    \"\"\"This parent node will invoke the subgraph.\"\"\"\n",
    "    global counter_parent_node\n",
    "\n",
    "    counter_parent_node += 1 # This code will run again on resuming!\n",
    "    print(f\"Entered `parent_node` a total of {counter_parent_node} times\")\n",
    "\n",
    "    # Please note that we're intentionally incrementing the state counter\n",
    "    # in the graph state as well to demonstrate that the subgraph update\n",
    "    # of the same key will not conflict with the parent graph (until\n",
    "    subgraph_state = subgraph.invoke(state)\n",
    "    return subgraph_state\n",
    "\n",
    "\n",
    "builder = StateGraph(State)\n",
    "builder.add_node(\"parent_node\", parent_node)\n",
    "builder.add_edge(START, \"parent_node\")\n",
    "\n",
    "# A checkpointer must be enabled for interrupts to work!\n",
    "checkpointer = MemorySaver()\n",
    "graph = builder.compile(checkpointer=checkpointer)\n",
    "\n",
    "config = {\n",
    "    \"configurable\": {\n",
    "      \"thread_id\": uuid.uuid4(),\n",
    "    }\n",
    "}\n",
    "\n",
    "for chunk in graph.stream({\"state_counter\": 1}, config):\n",
    "    print(chunk)\n",
    "\n",
    "print('--- Resuming ---')\n",
    "\n",
    "for chunk in graph.stream(Command(resume=\"Lưu Trọng Dũng\"), config):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078fdc84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lỗi kết nối: (psycopg.OperationalError) [Errno 11003] getaddrinfo failed\n",
      "(Background on this error at: https://sqlalche.me/e/20/e3q8)\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import create_engine\n",
    "\n",
    "DB_PASSWORD = \"Matkhaula@123\"\n",
    "DATABASE_URL = f\"postgresql+psycopg://postgres.rcvgpqodvckwzzrfbxml:{DB_PASSWORD}@aws-1-ap-southeast-1.pooler.supabase.com:6543/postgres\"\n",
    "\n",
    "engine = create_engine(DATABASE_URL)\n",
    "\n",
    "try:\n",
    "    with engine.connect() as conn:\n",
    "        result = conn.execute(\"SELECT 1\")\n",
    "        print(\"Kết nối thành công:\", result.fetchone())\n",
    "except Exception as e:\n",
    "    print(\"Lỗi kết nối:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a34de8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "from langchain_core.runnables.graph import MermaidDrawMethod\n",
    "from IPython.display import display, Image\n",
    "\n",
    "chat = ChatAgent().get_builder().compile()\n",
    "\n",
    "display(\n",
    "    Image(\n",
    "        chat.get_graph().draw_mermaid_png(\n",
    "            draw_method=MermaidDrawMethod.API,\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a130b7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "    async def process(self, state: State) -> State:\n",
    "        state[\"current_agent\"] = \"analysis\"\n",
    "\n",
    "        current_step = state[\"agent_logs\"][-1][\"step\"] + 1 if state[\"agent_logs\"] else 1\n",
    "        start_time = time()\n",
    "\n",
    "        result_msg = await self._chain.ainvoke({\"messages\": state[\"messages\"]})\n",
    "        result_text = getattr(result_msg, \"content\", str(result_msg))\n",
    "\n",
    "        end_time = time()\n",
    "        duration = end_time - start_time\n",
    "\n",
    "        state[\"agent_logs\"].append(\n",
    "            {\n",
    "                \"agent_name\": \"analysis\",\n",
    "                \"task\": \"Analyze user messages\",\n",
    "                \"result\": result_text,\n",
    "                \"step\": current_step,\n",
    "                \"start_time\": start_time,\n",
    "                \"end_time\": end_time,\n",
    "                \"duration\": duration,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        state[\"messages\"].append({\"role\": \"assistant\", \"content\": result_text})\n",
    "\n",
    "        return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e2e889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analysis\n",
      "\n",
      "\n",
      "supervisor (0.68s): assignment for analysis\n",
      "Here\n",
      "'s a breakdown of how to analyze the user's request:\n",
      "\n",
      "1. **\n",
      "Identify the core question:** The user wants to understand the concept of a derivative.\n",
      "\n",
      "2. **Identify the desired output:** The user wants a definition of derivative and example calculations.\n",
      "3. **Outline the steps to fulfill the request:**\n",
      "\n",
      "    *   Explain the definition of a derivative (conceptually and mathematically).\n",
      "    *   Provide a few example functions.\n",
      "    *   Demonstrate the\n",
      " calculation of the derivative for each example function, showing the steps.\n",
      "\n",
      "analysis (1.45s): solved\n",
      "NO\n",
      "_MATH\n",
      "\n",
      "\n",
      "logic (0.77s): skipped\n",
      "Okay\n",
      ", I can\n",
      " definitely help with that! Let's break down the concept of a derivative and then walk\n",
      " through some examples.\n",
      "\n",
      "**What is a Derivative?**\n",
      "\n",
      "At its heart\n",
      ", the derivative of a function tells you the *instantaneous rate of change* of that function at a specific point.  Think of it like this:\n",
      "\n",
      "*   \n",
      "**Imagine you're driving a car.** Your speed is how quickly your position (distance traveled) is changing with respect to time.  Your speedometer isn't showing\n",
      " your *average* speed over a long period, but your speed at *this very moment*.  The derivative is like that instantaneous speed.\n",
      "\n",
      "*   **Graphically:** If you plot a function on a graph, the derivative at a point is\n",
      " the slope of the line that's tangent to the curve at that point. A tangent line just \"kisses\" the curve at that specific location.\n",
      "\n",
      "**The Mathematical Definition**\n",
      "\n",
      "The formal definition of the derivative of a function *f\n",
      "(x)*, denoted as *f'(x)* (read as \"f prime of x\"), is:\n",
      "\n",
      "f'(x) = lim (h -> 0)  [f(x + h) - f(x)] / h\n",
      "\n",
      "Let's break that down:\n",
      "\n",
      "*   **f(\n",
      "x + h):**  This means you're taking a point *x* and adding a tiny amount *h* to it, then plugging that into your function.\n",
      "*   **f(x + h) - f(x):** This is the *change* in the function's value as you\n",
      " move from *x* to *x + h*.  It's the \"rise\" in the slope calculation.\n",
      "*   **h:** This is the tiny change in *x*. It's the \"run\" in the slope calculation.\n",
      "*   **[f(x + h) - f(x\n",
      ")] / h:** This is the slope of a line connecting the points (x, f(x)) and (x + h, f(x + h)) on the graph of the function.  This is called a *secant line*.\n",
      "*   **lim (h -> 0):** This is the\n",
      " crucial part. We're taking the limit as *h* gets infinitely small (approaches zero).  As *h* shrinks towards zero, that secant line becomes closer and closer to the tangent line at the point *x*.  The limit *is* the slope of the tangent line, and thus the\n",
      " derivative.\n",
      "\n",
      "**Examples and Derivative Calculations**\n",
      "\n",
      "Let's look at a few examples to make this more concrete.  I'll show how to find the derivative using the definition above.  (Note: There are faster rules for finding derivatives once you get the hang of it, but for now, we'll stick\n",
      " with the definition.)\n",
      "\n",
      "**Example 1:  f(x) = x<sup>2</sup>**\n",
      "\n",
      "1.  **Apply the definition:**\n",
      "\n",
      "    f'(x) = lim (h -> 0)  [ (x + h)<sup>2</sup> - x<sup>2</sup> ] / h\n",
      "\n",
      "2.  **\n",
      "Expand (x + h)<sup>2</sup>:**\n",
      "\n",
      "    f'(x) = lim (h -> 0)  [ x<sup>2</sup> + 2xh + h<sup>2</sup> - x<sup>2</sup> ] / h\n",
      "\n",
      "3.  **Simplify:**\n",
      "\n",
      "    f'(x) = lim (h -> \n",
      "0)  [ 2xh + h<sup>2</sup> ] / h\n",
      "\n",
      "4.  **Factor out an h from the numerator:**\n",
      "\n",
      "    f'(x) = lim (h -> 0)  [ h(2x + h) ] / h\n",
      "\n",
      "5.  **Cancel the h's:**\n",
      "\n",
      "\n",
      "    f'(x) = lim (h -> 0)  [ 2x + h ]\n",
      "\n",
      "6.  **Take the limit as h approaches 0:**\n",
      "\n",
      "    f'(x) = 2x + 0 = 2x\n",
      "\n",
      "Therefore, the derivative of f(x) =\n",
      " x<sup>2</sup> is f'(x) = 2x. This means that the slope of the tangent line to the curve *y = x<sup>2</sup>* at any point *x* is *2x*.\n",
      "\n",
      "**Example 2: f(x) = 3x + 1**\n",
      "\n",
      "\n",
      "1.  **Apply the definition:**\n",
      "\n",
      "    f'(x) = lim (h -> 0)  [ 3(x + h) + 1 - (3x + 1) ] / h\n",
      "\n",
      "2.  **Distribute:**\n",
      "\n",
      "    f'(x) = lim (h -> 0)\n",
      "  [ 3x + 3h + 1 - 3x - 1 ] / h\n",
      "\n",
      "3.  **Simplify:**\n",
      "\n",
      "    f'(x) = lim (h -> 0)  [ 3h ] / h\n",
      "\n",
      "4.  **Cancel the h's:**\n",
      "\n",
      "    \n",
      "f'(x) = lim (h -> 0)  [ 3 ]\n",
      "\n",
      "5.  **Take the limit as h approaches 0:**\n",
      "\n",
      "    f'(x) = 3\n",
      "\n",
      "Therefore, the derivative of f(x) = 3x + 1 is f'(x) = \n",
      "3.  This makes sense because *f(x) = 3x + 1* is a straight line with a constant slope of 3.  The derivative *is* the slope, and it's constant for all values of *x*.\n",
      "\n",
      "**Example 3: f(x) = c\n",
      " (where c is a constant)**\n",
      "\n",
      "1.  **Apply the definition:**\n",
      "\n",
      "    f'(x) = lim (h -> 0) [c - c] / h\n",
      "\n",
      "2.  **Simplify:**\n",
      "\n",
      "     f'(x) = lim (h -> 0) 0 / h\n",
      "\n",
      "3.  **\n",
      "Take the limit:**\n",
      "\n",
      "    f'(x) = 0\n",
      "\n",
      "Therefore, the derivative of a constant function is always 0. This is because a constant function doesn't change its value, so its rate of change is zero.\n",
      "\n",
      "**Key Takeaways**\n",
      "\n",
      "*   The derivative is the instantaneous rate of\n",
      " change of a function.\n",
      "*   Geometrically, the derivative is the slope of the tangent line.\n",
      "*   The formal definition involves a limit as *h* approaches zero.\n",
      "*   Finding derivatives using the definition can be a bit tedious, but it's important to understand the underlying concept.\n",
      "\n",
      "I\n",
      " hope this helps!  Do you have any specific functions you'd like me to find the derivative of, or any particular aspect of the derivative that you'd like me to explain in more detail?\n",
      "\n",
      "writer (8.38s): solved\n"
     ]
    }
   ],
   "source": [
    "from typing import Sequence\n",
    "from langchain_core.tools.base import BaseTool\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from typing import Sequence\n",
    "from time import time\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.tools.base import BaseTool\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langgraph.graph import MessagesState\n",
    "from typing import TypedDict\n",
    "from pydantic import Field\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from time import time\n",
    "import json \n",
    "\n",
    "GOOGLE_API_KEY = \"AIzaSyBbZBWB-7R5_uOI4_WM73G-cl3LWUZvsUg\"\n",
    "\n",
    "class AgentLog(TypedDict):\n",
    "    agent_name: str\n",
    "    task: str\n",
    "    result: str\n",
    "    step: int\n",
    "    start_time: float\n",
    "    end_time: float\n",
    "    duration: float\n",
    "\n",
    "\n",
    "class State(MessagesState):\n",
    "    thread_id: str\n",
    "    agent_logs: list[AgentLog] = Field(default_factory=list)\n",
    "    next_agent: str\n",
    "    prev_agent: str\n",
    "    task: str\n",
    "    human: False\n",
    "\n",
    "\n",
    "class BaseAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        agent_name: str,\n",
    "        tools: Sequence[BaseTool] | None = None,\n",
    "        model: object | None = None,\n",
    "    ) -> None:\n",
    "        self._tools = list(tools or [])\n",
    "        self._agent_name = agent_name\n",
    "\n",
    "        self._model = ChatGoogleGenerativeAI(\n",
    "            model=\"gemini-2.0-flash\",\n",
    "            google_api_key=GOOGLE_API_KEY,\n",
    "            disable_streaming=False,\n",
    "        ).bind_tools(self._tools)\n",
    "\n",
    "    async def process(self, state: State) -> State:\n",
    "        return state\n",
    "\n",
    "    def response_filter(self, content: str):\n",
    "        lines = content.strip().splitlines()\n",
    "        last_line = lines[-1].strip()\n",
    "\n",
    "        human_dict = json.loads(last_line.lower())\n",
    "        clean_content = \"\\n\".join(lines[:-1]).strip()\n",
    "        return (clean_content, human_dict[\"human\"])\n",
    "\n",
    "    def human_node(self, state: State) -> State:\n",
    "        if state[\"human\"]:\n",
    "            print(\"++++++++++++++++++++++++++++++++++++\")\n",
    "            interrupt({\"AIMessage\": state[\"task\"]})\n",
    "        else:\n",
    "            return state\n",
    "\n",
    "    def get_graph(self) -> StateGraph:\n",
    "        graph = StateGraph(State)\n",
    "        graph.add_node(self._agent_name, self.process)\n",
    "        graph.add_node(\"human_node\", self.human_node)\n",
    "\n",
    "        if self._tools:\n",
    "            graph.add_node(\"tools\", ToolNode(self._tools))\n",
    "            graph.add_conditional_edges(\n",
    "                self._agent_name,\n",
    "                tools_condition,\n",
    "                {\"tools\": \"tools\", \"__end__\": \"human_node\"},\n",
    "            )\n",
    "            graph.add_edge(\"tools\", self._agent_name)\n",
    "        else:\n",
    "            graph.add_edge(self._agent_name, \"human_node\")\n",
    "\n",
    "        graph.set_entry_point(self._agent_name)\n",
    "        graph.set_finish_point(\"human_node\")\n",
    "        return graph.compile(name=self._agent_name)\n",
    "\n",
    "\n",
    "class AnalysisAgent(BaseAgent):\n",
    "    def __init__(self, tools: Sequence[BaseTool] | None = None) -> None:\n",
    "        super().__init__(\n",
    "            agent_name=\"analysis\",\n",
    "            tools=tools or [],\n",
    "            model=None,\n",
    "        )\n",
    "\n",
    "        self._prompt = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                SystemMessage(\n",
    "                    content=\"\"\"\n",
    "                You are the ANALYSIS agent.\n",
    "                Your task: analyze the user's request only.\n",
    "                Be concise, structured, and focus on reasoning or problem-solving steps.\n",
    "                Do not generate the final answer or solution — only break down the request into what needs to be done.\n",
    "                \"\"\"\n",
    "                ),\n",
    "                MessagesPlaceholder(\"task\"),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self._chain = self._prompt | self._model\n",
    "\n",
    "    async def process(self, state: State) -> State:\n",
    "        start = time()\n",
    "        result_msg = await self._chain.ainvoke(\n",
    "            {\"task\": [HumanMessage(content=state[\"task\"])]}\n",
    "        )\n",
    "        end = time()\n",
    "        duration = end - start\n",
    "        state[\"task\"] = result_msg.content\n",
    "        print(f\"analysis ({duration:.02f}s): solved\")\n",
    "        state[\"next_agent\"] = \"logic\"\n",
    "        state[\"prev_agent\"] = \"analysis\"\n",
    "        return state\n",
    "\n",
    "class WriterAgent(BaseAgent):\n",
    "    def __init__(self, tools: Sequence[BaseTool] | None = None) -> None:\n",
    "        super().__init__(\n",
    "            agent_name=\"writer\",\n",
    "            tools=tools or [],\n",
    "            model=None,\n",
    "        )\n",
    "\n",
    "        self._prompt = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                SystemMessage(content=\"\"\"  \n",
    "                    You are the WRITER agent.\n",
    "                    You should use the results from the previous agent's step and continue processing.\n",
    "                    Your job: respond naturally and helpfully, based on the prior analysis or direct user message.\n",
    "                \"\"\"),\n",
    "                \n",
    "                MessagesPlaceholder(\"task\"),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self._chain = self._prompt | self._model\n",
    "\n",
    "    async def process(self, state: State) -> State:\n",
    "        start = time()\n",
    "        result_msg = None\n",
    "        if state[\"prev_agent\"] != \"supervisor\":\n",
    "            result_msg = await self._chain.ainvoke(\n",
    "                {\"task\": [HumanMessage(content=state[\"task\"])]}\n",
    "            )\n",
    "        elif state[\"prev_agent\"] == \"supervisor\":\n",
    "            result_msg = await self._chain.ainvoke(\n",
    "                {\"task\": [HumanMessage(content=state[\"task\"])]}\n",
    "            )\n",
    "        end = time()\n",
    "        duration = end - start\n",
    "        print(f\"writer ({duration:.02f}s): solved\")\n",
    "        return {\"messages\": [result_msg]}\n",
    "\n",
    "\n",
    "class LogicAgent(BaseAgent):\n",
    "    def __init__(self, tools: Sequence[BaseTool] | None = None) -> None:\n",
    "        super().__init__(\n",
    "            agent_name=\"logic\",\n",
    "            tools=tools or [],\n",
    "            model=None,\n",
    "        )\n",
    "\n",
    "        self._prompt = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                SystemMessage(\n",
    "                    content=\"\"\"\n",
    "                You are the LOGIC/MATH agent.\n",
    "                You should use the results from the previous agent's step and continue processing.\n",
    "                Goal:\n",
    "                - If the user's request clearly involves mathematics (arithmetic, algebra, calculus, probability, statistics, unit conversion, dimensional analysis, etc.), then you must SOLVE it carefully following the rules below.\n",
    "                - If the request DOES NOT contain any mathematical or logical problems, do not attempt to answer. Instead, return NO_MATH.\n",
    "                This tag indicates the supervisor should route the request to another agent.\n",
    "\n",
    "                Rules for solving math:\n",
    "                1) Work carefully and compute digit-by-digit where needed; keep units and significant figures consistent.\n",
    "                2) State minimal assumptions only when data is missing/ambiguous; never invent data.\n",
    "                3) Prefer exact forms (fractions, radicals) unless a decimal is requested; if rounding, specify the rule.\n",
    "                4) Provide a brief structured result:\n",
    "                - Answer: <final value with units if any>\n",
    "                - Justification: 2-4 concise steps (no long chain-of-thought)\n",
    "                - Check: a quick verification (plug back / dimension / sanity)\n",
    "                5) If multiple sub-parts, label (a), (b), (c) clearly.\n",
    "                6) Mirror the user's language (Vietnamese or English).\n",
    "                7) Do not use external tools or the web.\n",
    "                \"\"\"\n",
    "                ),\n",
    "                MessagesPlaceholder(\"task\"),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self._chain = self._prompt | self._model\n",
    "\n",
    "    async def process(self, state: State) -> State:\n",
    "        start = time()\n",
    "        result_msg = await self._chain.ainvoke(\n",
    "            {\"task\": [HumanMessage(content=state[\"task\"])]}\n",
    "        )\n",
    "        end = time()\n",
    "        duration = end - start\n",
    "\n",
    "        if \"NO_MATH\" in result_msg.content:\n",
    "            print(f\"logic ({duration:.02f}s): skipped\")\n",
    "            state[\"task\"] = state[\"task\"]\n",
    "        else:\n",
    "            print(f\"logic ({duration:.02f}s): solved\")\n",
    "            state[\"task\"] = result_msg.content\n",
    "        state[\"next_agent\"] = \"writer\"\n",
    "        state[\"prev_agent\"] = \"logic\"\n",
    "        return state\n",
    "\n",
    "\n",
    "class SupervisorAgent(BaseAgent):\n",
    "    def __init__(self, tools: Sequence[BaseTool] | None = None) -> None:\n",
    "        super().__init__(\n",
    "            agent_name=\"supervisor\",\n",
    "            tools=tools or [],\n",
    "            model=None,\n",
    "        )\n",
    "\n",
    "        self._prompt = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\n",
    "                    \"system\",\n",
    "                    \"You are the SUPERVISOR agent.\\n\"\n",
    "                    \"Your job: decide which specialized agent should handle the user's last message.\\n\\n\"\n",
    "                    \"Rules:\\n\"\n",
    "                    \"- If the request is casual conversation, chit-chat, or simple → respond ONLY with: writer\\n\"\n",
    "                    \"- If the request is complex, requires reasoning, problem-solving, or analysis → respond ONLY with: analysis\\n\\n\"\n",
    "                    \"IMPORTANT: Output exactly one word: 'writer' or 'analysis'.\",\n",
    "                ),\n",
    "                MessagesPlaceholder(\"assignment\"),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self._chain = self._prompt | self._model\n",
    "\n",
    "    async def process(self, state: State) -> State:\n",
    "        start = time()\n",
    "        result_msg = await self._chain.ainvoke({\"assignment\": [state[\"messages\"][-1]]})\n",
    "        end = time()\n",
    "        duration = end - start\n",
    "        state[\"next_agent\"] = result_msg.content.strip().lower()\n",
    "        state[\"task\"] = state[\"messages\"][-1].content\n",
    "        state[\"prev_agent\"] = \"supervisor\"\n",
    "        print(f\"supervisor ({duration:.02f}s): assignment for {state[\"next_agent\"]}\")\n",
    "        return state\n",
    "\n",
    "def route(state: State) -> str:\n",
    "    if state[\"next_agent\"] == \"analysis\":\n",
    "        return \"analysis\"\n",
    "    elif state[\"next_agent\"] == \"writer\":\n",
    "        return \"writer\"\n",
    "\n",
    "\n",
    "app = StateGraph(State)\n",
    "\n",
    "supervisor = SupervisorAgent()\n",
    "analysis = AnalysisAgent()\n",
    "writer = WriterAgent()\n",
    "logic = LogicAgent()\n",
    "\n",
    "app.add_node(\"supervisor\", supervisor.get_builder().compile())\n",
    "app.add_node(\"analysis\", analysis.get_builder().compile())\n",
    "app.add_node(\"writer\", writer.get_builder().compile())\n",
    "app.add_node(\"logic\", logic.get_builder().compile())\n",
    "\n",
    "app.set_entry_point(\"supervisor\")\n",
    "app.add_conditional_edges(\"supervisor\", route, {\"analysis\": \"analysis\", \"writer\":\"writer\"})\n",
    "app.add_edge(\"analysis\", \"logic\")\n",
    "app.add_edge(\"logic\",\"writer\")\n",
    "app.set_finish_point(\"writer\")\n",
    "\n",
    "workflow = app.compile(checkpointer=MemorySaver())\n",
    "\n",
    "input_state = {\n",
    "    \"messages\": [\n",
    "        HumanMessage(content=\"đạo hàm là gi và ví dụ tính toán cụ thể\"),\n",
    "    ],\n",
    "    \"thread_id\": \"123\",\n",
    "    \"agent_logs\": [\n",
    "        {\n",
    "            \"agent_name\": \"supervisor\",\n",
    "            \"task\": None,\n",
    "            \"result\": None,\n",
    "            \"step\": 0,\n",
    "            \"start_time\": None,\n",
    "            \"end_time\": None,\n",
    "            \"duration\": None,\n",
    "        }\n",
    "    ],\n",
    "    \"next_agent\": None,\n",
    "    \"prev_agent\": None,\n",
    "    \"task\": \"\",\n",
    "}\n",
    "config = {\"configurable\": {\"thread_id\": \"123\"}}\n",
    "async for data in workflow.astream(\n",
    "            input_state,\n",
    "            config=config,\n",
    "            stream_mode=[\"messages\"],\n",
    "        ):\n",
    "            data_type, (msg, meta_date) = data\n",
    "            if data_type == \"messages\":\n",
    "                print(msg.content)\n",
    "# (\n",
    "#     AIMessageChunk(\n",
    "#         content=\"ệt vời! Bạn muốn tôi tả một dòng sông như thế nào? Để tôi viết\",\n",
    "#         additional_kwargs={},\n",
    "#         response_metadata={\"safety_ratings\": []},\n",
    "#         id=\"run--e71ebc3d-bbba-4737-aea3-61c1e273329c\",\n",
    "#         usage_metadata={\n",
    "#             \"input_tokens\": 0,\n",
    "#             \"output_tokens\": 0,\n",
    "#             \"total_tokens\": 0,\n",
    "#             \"input_token_details\": {\"cache_read\": 0},\n",
    "#         },\n",
    "#     ),\n",
    "#     {\n",
    "#         \"thread_id\": \"123\",\n",
    "#         \"langgraph_step\": 1,\n",
    "#         \"langgraph_node\": \"writer\",\n",
    "#         \"langgraph_triggers\": (\"branch:to:writer\",),\n",
    "#         \"langgraph_path\": (\"__pregel_pull\", \"writer\"),\n",
    "#         \"langgraph_checkpoint_ns\": \"writer:0dbf21d6-2ddb-bf8e-fd2d-28e162e0965f|writer:01b812c0-3585-a730-9ff1-cba8f76a0c99\",\n",
    "#         \"checkpoint_ns\": \"writer:0dbf21d6-2ddb-bf8e-fd2d-28e162e0965f\",\n",
    "#         \"ls_provider\": \"google_genai\",\n",
    "#         \"ls_model_name\": \"gemini-2.0-flash\",\n",
    "#         \"ls_model_type\": \"chat\",\n",
    "#         \"ls_temperature\": 0.7,\n",
    "#     },\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47ebf41b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e:\\Project\\chatbot\n",
      "('messages', (AIMessageChunk(content='writer', additional_kwargs={}, response_metadata={'safety_ratings': []}, id='run--f15de017-ea1a-42eb-aea8-b1195c8743ae', usage_metadata={'input_tokens': 115, 'output_tokens': 0, 'total_tokens': 115, 'input_token_details': {'cache_read': 0}}), {'thread_id': '123', 'langgraph_step': 1, 'langgraph_node': 'assigner', 'langgraph_triggers': ('branch:to:assigner',), 'langgraph_path': ('__pregel_pull', 'assigner'), 'langgraph_checkpoint_ns': 'assigner:1ee1e0bb-a779-a097-8c8a-1c17518443d7|assigner:2bdb7ee3-f2f5-0853-58f7-a2b3e845173e', 'checkpoint_ns': 'assigner:1ee1e0bb-a779-a097-8c8a-1c17518443d7', 'ls_provider': 'google_genai', 'ls_model_name': 'gemini-2.0-flash', 'ls_model_type': 'chat', 'ls_temperature': 0.7}))\n",
      "('messages', (AIMessageChunk(content='', additional_kwargs={}, response_metadata={'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--f15de017-ea1a-42eb-aea8-b1195c8743ae', usage_metadata={'input_tokens': -3, 'output_tokens': 1, 'total_tokens': -2, 'input_token_details': {'cache_read': 0}}), {'thread_id': '123', 'langgraph_step': 1, 'langgraph_node': 'assigner', 'langgraph_triggers': ('branch:to:assigner',), 'langgraph_path': ('__pregel_pull', 'assigner'), 'langgraph_checkpoint_ns': 'assigner:1ee1e0bb-a779-a097-8c8a-1c17518443d7|assigner:2bdb7ee3-f2f5-0853-58f7-a2b3e845173e', 'checkpoint_ns': 'assigner:1ee1e0bb-a779-a097-8c8a-1c17518443d7', 'ls_provider': 'google_genai', 'ls_model_name': 'gemini-2.0-flash', 'ls_model_type': 'chat', 'ls_temperature': 0.7}))\n",
      "('updates', {'assigner': {'messages': [HumanMessage(content='ví dụ về đạo hàm', additional_kwargs={}, response_metadata={}, id='d5f03595-cc2d-487a-83a9-a3a808f7c7ef'), HumanMessage(content='hello', additional_kwargs={}, response_metadata={}, id='2c3f8783-d7ca-4b16-a0c0-4e1bf096e16e')], 'thread_id': '123', 'agent_logs': [{'agent_name': 'supervisor', 'task': None, 'result': None, 'step': 0, 'start_time': None, 'end_time': None, 'duration': None}, {'agent_name': 'assigner', 'task': 'hello', 'result': 'writer', 'start_time': 1757937432.3862572, 'end_time': 1757937432.9114635, 'duration': 0.5252063274383545}], 'next_agent': 'writer', 'prev_agent': 'assigner', 'task': 'hello', 'human': False}})\n",
      "('messages', (AIMessageChunk(content='Chào', additional_kwargs={}, response_metadata={'safety_ratings': []}, id='run--4e3eee03-9876-427d-8805-04c6c95405e0', usage_metadata={'input_tokens': 119, 'output_tokens': 0, 'total_tokens': 119, 'input_token_details': {'cache_read': 0}}), {'thread_id': '123', 'langgraph_step': 1, 'langgraph_node': 'writer', 'langgraph_triggers': ('branch:to:writer',), 'langgraph_path': ('__pregel_pull', 'writer'), 'langgraph_checkpoint_ns': 'writer:76e371a0-5db1-a2e6-c30c-19c71b8f90cf|writer:962b8555-69b0-c2d4-8c85-05df6dac0816', 'checkpoint_ns': 'writer:76e371a0-5db1-a2e6-c30c-19c71b8f90cf', 'ls_provider': 'google_genai', 'ls_model_name': 'gemini-2.0-flash', 'ls_model_type': 'chat', 'ls_temperature': 0.7}))\n",
      "('messages', (AIMessageChunk(content=' bạn! Bạn có điều gì muốn hỏi không?\\n', additional_kwargs={}, response_metadata={'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--4e3eee03-9876-427d-8805-04c6c95405e0', usage_metadata={'input_tokens': -6, 'output_tokens': 13, 'total_tokens': 7, 'input_token_details': {'cache_read': 0}}), {'thread_id': '123', 'langgraph_step': 1, 'langgraph_node': 'writer', 'langgraph_triggers': ('branch:to:writer',), 'langgraph_path': ('__pregel_pull', 'writer'), 'langgraph_checkpoint_ns': 'writer:76e371a0-5db1-a2e6-c30c-19c71b8f90cf|writer:962b8555-69b0-c2d4-8c85-05df6dac0816', 'checkpoint_ns': 'writer:76e371a0-5db1-a2e6-c30c-19c71b8f90cf', 'ls_provider': 'google_genai', 'ls_model_name': 'gemini-2.0-flash', 'ls_model_type': 'chat', 'ls_temperature': 0.7}))\n",
      "('updates', {'writer': {'messages': [HumanMessage(content='ví dụ về đạo hàm', additional_kwargs={}, response_metadata={}, id='d5f03595-cc2d-487a-83a9-a3a808f7c7ef'), HumanMessage(content='hello', additional_kwargs={}, response_metadata={}, id='2c3f8783-d7ca-4b16-a0c0-4e1bf096e16e'), AIMessage(content='Chào bạn! Bạn có điều gì muốn hỏi không?\\n', additional_kwargs={}, response_metadata={'safety_ratings': [], 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash'}, id='run--4e3eee03-9876-427d-8805-04c6c95405e0', usage_metadata={'input_tokens': 113, 'output_tokens': 13, 'total_tokens': 126, 'input_token_details': {'cache_read': 0}})], 'thread_id': '123', 'agent_logs': [{'agent_name': 'supervisor', 'task': None, 'result': None, 'step': 0, 'start_time': None, 'end_time': None, 'duration': None}, {'agent_name': 'assigner', 'task': 'hello', 'result': 'writer', 'start_time': 1757937432.3862572, 'end_time': 1757937432.9114635, 'duration': 0.5252063274383545}, {'agent_name': 'writer', 'task': 'hello', 'result': 'Chào bạn! Bạn có điều gì muốn hỏi không?\\n', 'step': 2, 'start_time': 1757937432.9273744, 'end_time': 1757937433.7168922, 'duration': 0.789517879486084}], 'next_agent': 'writer', 'prev_agent': 'assigner', 'task': 'hello', 'human': False}})\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables.graph import MermaidDrawMethod\n",
    "from IPython.display import display, Image\n",
    "import sys, os\n",
    "from langchain_core.messages import HumanMessage\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\",\"..\")))\n",
    "print(os.path.abspath(os.path.join(os.getcwd(),\"..\",\"..\")))\n",
    "from src.agents.workflow import graph\n",
    "from src.agents.analyst.analyst import AnalystAgent\n",
    "from src.agents.writer.writer import WriterAgent\n",
    "from src.agents.assigner.assigner import AssignerAgent\n",
    "from src.agents.workflow import graph\n",
    "# display(\n",
    "#     Image(\n",
    "#         graph.get_graph().draw_mermaid_png(\n",
    "#             draw_method=MermaidDrawMethod.API,\n",
    "#         ),\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# graph1 = AssignerAgent()\n",
    "# display(\n",
    "#     Image(\n",
    "#         graph1.get_graph().get_graph().draw_mermaid_png(\n",
    "#             draw_method=MermaidDrawMethod.API,\n",
    "#         ),\n",
    "#     )\n",
    "# )\n",
    "input_state = {\n",
    "    \"messages\": [\n",
    "        HumanMessage(content=\"hello\"),\n",
    "    ],\n",
    "    \"thread_id\": \"123\",\n",
    "    \"agent_logs\": [\n",
    "        {\n",
    "            \"agent_name\": \"supervisor\",\n",
    "            \"task\": None,\n",
    "            \"result\": None,\n",
    "            \"step\": 0,\n",
    "            \"start_time\": None,\n",
    "            \"end_time\": None,\n",
    "            \"duration\": None,\n",
    "        }\n",
    "    ],\n",
    "    \"next_agent\": None,\n",
    "    \"prev_agent\": None,\n",
    "    \"task\": \"\",\n",
    "}\n",
    "from langgraph.types import interrupt\n",
    "config = {\"configurable\": {\"thread_id\": \"123\"}}\n",
    "async for event in graph.astream(\n",
    "    input_state,\n",
    "    config=config,\n",
    "    stream_mode=[\"messages\",\"updates\"],\n",
    "):\n",
    "    data_type, payload = event\n",
    "    print(event)\n",
    "    if data_type == \"updates\":\n",
    "        type, state_data = next(reversed(payload.items()))\n",
    "        \n",
    "        if type == \"__interrupt__\":\n",
    "            print(state_data[0].value[\"AIMessage\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d8ec72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "supervisor (0.62s): assignment for writer\n",
      "writer (0.63s):\n",
      " , how can I help you today?\n"
     ]
    }
   ],
   "source": [
    "res = await workflow.ainvoke(\n",
    "    {\"messages\": [HumanMessage(content=\"hello\")]},\n",
    "    config={\"configurable\": {\"thread_id\": \"123\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b550a937",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='hello', additional_kwargs={}, response_metadata={}, id='39f8cd7c-ecdb-4728-8546-9f1213939de9'),\n",
       " AIMessage(content='Hey there! How can I help you today?', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--172a6047-251e-4d9d-a6b4-e492ba0b61e0-0', usage_metadata={'input_tokens': 27, 'output_tokens': 11, 'total_tokens': 38, 'input_token_details': {'cache_read': 0}})]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[\"messages\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf387ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "supervisor (0.68s): assignment for analysis\n",
      "analysis (0.75s): solved\n",
      "logic (2.79s): solved\n",
      "writer (3.70s): solved\n",
      "trả lời tiếng việt\n",
      "ví dụ 1 bài toán về machine learning và giải thích từng bước\n",
      "This is a great, comprehensive breakdown of the process for tackling a click-through rate (CTR) prediction problem! You've covered all the crucial steps, from defining the problem to monitoring the deployed model. The explanations are clear and the inclusion of specific techniques and examples within each step is very helpful.\n",
      "\n",
      "Here are a few of my thoughts and potential expansions, presented as natural follow-up questions or considerations:\n",
      "\n",
      "*   **Data Collection & Feature Understanding:** You mentioned gathering data on user demographics, browsing history, etc.  What are some potential challenges in collecting this data, and how might you address them (e.g., data privacy concerns, incomplete data, data silos)?  How would you prioritize which features to focus on initially? Would you perform any exploratory data analysis (EDA) to understand the relationships between features and the target variable *before* feature engineering?\n",
      "\n",
      "*   **Feature Engineering - Beyond the Basics:** You provided some good examples. What other, more advanced, feature engineering techniques might be considered for this problem? (e.g., using natural language processing (NLP) on ad copy or user search queries, creating interaction features between different variables, or using external data sources).\n",
      "\n",
      "*   **Model Selection - Considerations Beyond Accuracy:** You mentioned starting with a simpler model. Besides accuracy, what other factors might influence your model selection (e.g., interpretability, training time, deployment complexity, explainability for stakeholders)?  Are there specific business constraints that would favor one model over another?\n",
      "\n",
      "*   **Evaluation Metrics - Choosing the Right One:** You listed several evaluation metrics.  In a real-world CTR prediction scenario, which metric would likely be the most important to optimize for, and why?  (e.g., Is precision more important than recall if you want to avoid showing ads to users who are unlikely to click?)  How would you handle class imbalance (CTR is often very low)?\n",
      "\n",
      "*   **Deployment - Practical Considerations:** You mentioned deployment.  What are some common deployment architectures for a model like this? How would you handle the real-time prediction requirements? What are some tools and technologies commonly used for deploying machine learning models?\n",
      "\n",
      "*   **Monitoring - Detecting and Addressing Model Drift:** You mentioned monitoring performance. What are some key indicators of model drift (i.e., when the model's performance degrades over time)? How would you automatically detect and address model drift?\n",
      "\n",
      "Overall, this is an excellent overview. My questions above are just intended to prompt further thought and discussion on some of the more nuanced aspects of solving this type of machine learning problem.  What would you like to explore in more detail? Perhaps a deeper dive into feature engineering, model selection trade-offs, or deployment strategies?\n",
      "trả lời tiếng việt\n",
      "bài toán cụ thể hơn\n",
      "Lời giải của bạn gần đúng rồi, nhưng có một chút sai sót nhỏ trong bước kiểm tra. Sai số không phải do làm tròn, mà do kết quả giải phương trình có sai sót. Để mình giải lại chi tiết hơn nhé:\n",
      "\n",
      "**Giải:**\n",
      "\n",
      "1.  **Gọi ẩn:** Gọi quãng đường AB là x (km) (x > 0).\n",
      "2.  **Thời gian đi và về:**\n",
      "    *   Thời gian đi từ A đến B: x/12 (giờ)\n",
      "    *   Thời gian đi từ B về A: x/10 (giờ)\n",
      "3.  **Đổi đơn vị:** 30 phút = 0.5 giờ\n",
      "4.  **Lập phương trình:** Tổng thời gian cả đi lẫn về (bao gồm cả thời gian nghỉ) là 5 giờ. Vậy thời gian đi và về (không tính nghỉ) là: 5 - 0.5 = 4.5 (giờ). Ta có phương trình:\n",
      "\n",
      "    x/12 + x/10 = 4.5\n",
      "5.  **Giải phương trình:**\n",
      "    *   Quy đồng mẫu số: (5x + 6x) / 60 = 4.5\n",
      "    *   11x / 60 = 4.5\n",
      "    *   11x = 4.5 * 60\n",
      "    *   11x = 270\n",
      "    *   x = 270 / 11\n",
      "    *   x ≈ 24.55 (km)\n",
      "\n",
      "**Vậy, quãng đường AB là khoảng 24.55 km.**\n",
      "\n",
      "**Kiểm tra lại:**\n",
      "\n",
      "*   Thời gian đi: 24.55 km / 12 km/h ≈ 2.05 giờ\n",
      "*   Thời gian về: 24.55 km / 10 km/h ≈ 2.455 giờ\n",
      "*   Tổng thời gian (không tính nghỉ): 2.05 giờ + 2.455 giờ ≈ 4.505 giờ\n",
      "*   Tổng thời gian (tính nghỉ): 4.505 giờ + 0.5 giờ ≈ 5.005 giờ (sai số rất nhỏ do làm tròn)\n",
      "\n",
      "**Kết luận:**\n",
      "\n",
      "Quãng đường AB là khoảng **24.55 km**.  Kết quả ban đầu của bạn là 24 km chưa chính xác lắm.\n"
     ]
    }
   ],
   "source": [
    "res = await workflow.ainvoke(\n",
    "    {\"messages\": [SystemMessage(content=\"trả lời tiếng việt\"),HumanMessage(content=\"bài toán cụ thể hơn\")]},\n",
    "    config={\"configurable\": {\"thread_id\": \"123\"}},\n",
    ")\n",
    "\n",
    "for msg in res[\"messages\"]:\n",
    "    print(msg.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8adc1ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((), 'updates', {'node_1': {'foo': 'hi! foo'}})\n",
      "(('node_2:0af2105d-237e-4926-c55a-4e58383281e9',), 'updates', {'subgraph_node_1': {'bar': 'bar'}})\n",
      "(('node_2:0af2105d-237e-4926-c55a-4e58383281e9',), 'updates', {'subgraph_node_2': {'foo': 'hi! foobar'}})\n",
      "((), 'updates', {'node_2': {'foo': 'hi! foobar'}})\n"
     ]
    }
   ],
   "source": [
    "from langgraph.graph import START, StateGraph\n",
    "from typing import TypedDict\n",
    "\n",
    "\n",
    "# Define subgraph\n",
    "class SubgraphState(TypedDict):\n",
    "    foo: str  # note that this key is shared with the parent graph state\n",
    "    bar: str\n",
    "\n",
    "\n",
    "def subgraph_node_1(state: SubgraphState):\n",
    "    return {\"bar\": \"bar\"}\n",
    "\n",
    "\n",
    "def subgraph_node_2(state: SubgraphState):\n",
    "    return {\"foo\": state[\"foo\"] + state[\"bar\"]}\n",
    "\n",
    "\n",
    "subgraph_builder = StateGraph(SubgraphState)\n",
    "subgraph_builder.add_node(subgraph_node_1)\n",
    "subgraph_builder.add_node(subgraph_node_2)\n",
    "subgraph_builder.add_edge(START, \"subgraph_node_1\")\n",
    "subgraph_builder.add_edge(\"subgraph_node_1\", \"subgraph_node_2\")\n",
    "subgraph = subgraph_builder.compile()\n",
    "\n",
    "\n",
    "# Define parent graph\n",
    "class ParentState(TypedDict):\n",
    "    foo: str\n",
    "\n",
    "\n",
    "def node_1(state: ParentState):\n",
    "    return {\"foo\": \"hi! \" + state[\"foo\"]}\n",
    "\n",
    "\n",
    "builder = StateGraph(ParentState)\n",
    "builder.add_node(\"node_1\", node_1)\n",
    "builder.add_node(\"node_2\", subgraph)\n",
    "builder.add_edge(START, \"node_1\")\n",
    "builder.add_edge(\"node_1\", \"node_2\")\n",
    "graph = builder.compile()\n",
    "\n",
    "for chunk in graph.stream(\n",
    "    {\"foo\": \"foo\"},\n",
    "    stream_mode=[\"updates\",\"messages\"],\n",
    "    subgraphs=True,\n",
    "):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb427aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Run until interrupt ===\n",
      "ok\n",
      "Event: {'__interrupt__': (Interrupt(value={'text_to_revise': 'original text'}, resumable=True, ns=['human_node:79a99f9c-1a07-3b20-29fb-af9742ac7e9e']),)}\n",
      "State values: StateSnapshot(values={'some_text': 'original text'}, next=('human_node',), config={'configurable': {'thread_id': '3018111c-dfdd-48d4-a986-d53efb564270', 'checkpoint_ns': '', 'checkpoint_id': '1f09210c-0308-6c83-8000-4e0b235be942'}}, metadata={'source': 'loop', 'writes': None, 'step': 0, 'parents': {}, 'thread_id': '3018111c-dfdd-48d4-a986-d53efb564270'}, created_at='2025-09-15T08:48:29.002457+00:00', parent_config={'configurable': {'thread_id': '3018111c-dfdd-48d4-a986-d53efb564270', 'checkpoint_ns': '', 'checkpoint_id': '1f09210c-0306-655d-bfff-91e5de33288c'}}, tasks=(PregelTask(id='79a99f9c-1a07-3b20-29fb-af9742ac7e9e', name='human_node', path=('__pregel_pull', 'human_node'), error=None, interrupts=(Interrupt(value={'text_to_revise': 'original text'}, resumable=True, ns=['human_node:79a99f9c-1a07-3b20-29fb-af9742ac7e9e']),), state=None, result=None),), interrupts=(Interrupt(value={'text_to_revise': 'original text'}, resumable=True, ns=['human_node:79a99f9c-1a07-3b20-29fb-af9742ac7e9e']),))\n",
      "Interrupt: None\n",
      "=== Resume with edited text ===\n",
      "ok\n",
      "ok1\n",
      "Event: {'human_node': {'some_text': 'Edited text'}}\n"
     ]
    }
   ],
   "source": [
    "from typing import TypedDict\n",
    "import uuid\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langgraph.constants import START\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.types import interrupt, Command\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    some_text: str\n",
    "\n",
    "\n",
    "def human_node(state: State):\n",
    "    print(\"ok\")\n",
    "    value = interrupt({\"text_to_revise\": state[\"some_text\"]})\n",
    "    print(\"ok1\")\n",
    "    return {\"some_text\": value}\n",
    "\n",
    "\n",
    "# Build the graph\n",
    "graph_builder = StateGraph(State)\n",
    "graph_builder.add_node(\"human_node\", human_node)\n",
    "graph_builder.add_edge(START, \"human_node\")\n",
    "checkpointer = InMemorySaver()\n",
    "graph = graph_builder.compile(checkpointer=checkpointer)\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": str(uuid.uuid4())}}\n",
    "\n",
    "# --- Run until interrupt ---\n",
    "print(\"=== Run until interrupt ===\")\n",
    "for event in graph.stream({\"some_text\": \"original text\"}, config=config):\n",
    "    print(\"Event:\", event)\n",
    "\n",
    "# Lấy state khi bị interrupt\n",
    "snapshot = graph.get_state(config)\n",
    "print(\"State values:\", snapshot)\n",
    "print(\"Interrupt:\", snapshot.metadata.get(\"__interrupt__\"))\n",
    "\n",
    "# --- Resume ---\n",
    "print(\"=== Resume with edited text ===\")\n",
    "for event in graph.stream(Command(resume=\"Edited text\"), config=config):\n",
    "    print(\"Event:\", event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1781150",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
