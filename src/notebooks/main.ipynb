{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81e2962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from langchain.text_splitter import (\n",
    "    RecursiveCharacterTextSplitter,\n",
    "    CharacterTextSplitter,\n",
    ")\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "\n",
    "async def create_vector_db_from_file(file_path=\"test.pdf\"):\n",
    "\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    documents = await asyncio.to_thread(loader.load)\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200,\n",
    "        add_start_index=True,\n",
    "    )\n",
    "    texts = await asyncio.to_thread(text_splitter.split_documents, documents)\n",
    "    print(len(texts))\n",
    "    embedding = HuggingFaceEmbeddings(\n",
    "        model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    )\n",
    "\n",
    "    db = await asyncio.to_thread(FAISS.from_documents, texts, embedding)\n",
    "    print(db.as_retriever)\n",
    "await create_vector_db_from_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9471ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chương 4: Mảng, Chuỗi và con trỏ \n",
      " \n",
      "Phần 1: Lý thuyết: \n",
      "1. Lớp (Class) và Đối Tượng (Object) \n",
      "Class: Bản thiết kế của đối tượng, chứa thuộc tính (data) và phương \n",
      "thức (functions). \n",
      "Object: Thể hiện cụ thể của lớp. \n",
      "Ví dụ: \n",
      "class Student {private: \n",
      "  string name;     Thuộc tính private \n",
      "  int age;  public: \n",
      "   Constructor: Khởi tạo đối tượng \n",
      "  Student(string n, int a) { \n",
      "    name = n; \n",
      "    age = a; \n",
      "  } \n",
      " \n",
      "   Phương thức public \n",
      "  void display() { \n",
      "    cout << \"Ten: \" << name << \", Tuoi: \" << age << endl; \n",
      "  }}; \n",
      " Tạo đối tượng \n",
      "Student s1(\"Alice\", 20); \n",
      "s1.display();   Output: Ten: Alice, Tuoi: 20 \n",
      "Constructor/Destructor: \n",
      "Constructor: Khởi tạo giá trị mặc định khi đối tượng được tạo. \n",
      "Destructor: Hủy đối tượng, giải phóng bộ nhớ (kí hiệu ~). \n",
      "~Student() { \n",
      "  cout << \"Doi tuong da bi huy\";} \n",
      " \n",
      "2. Tính Kế Thừa (Inheritance) \n",
      "Kế thừa: Lớp con (derived class) kế thừa thuộc tính/phương thức từ lớp cha \n",
      "(base class). \n",
      "Ví dụ: \n",
      "class Person {protected: \n",
      "  string name;public: \n",
      "  void setName(string n) { name = n; }}; \n",
      " Lớp Student kế thừa từ Personclass Student : public Person {private: \n",
      "  int studentId;public: \n",
      "  void setStudentId(int id) { studentId = id; } \n",
      "  void display() { \n",
      "    cout << \"Ten: \" << name << \", ID: \" << studentId << endl; \n",
      "  }}; \n",
      " \n",
      "3. Tính Đa Hình (Polymorphism) \n",
      "Virtual Function: Cho phép ghi đè (override) phương thức ở lớp con. \n",
      "class Animal {public: \n",
      "  virtual void sound() {   Hàm ảo \n",
      "    cout << \"Tieng keu\" << endl; \n",
      "  }}; \n",
      "class Dog : public Animal {public: \n",
      "  void sound() override {   Ghi đè \n",
      "    cout << \"Gau gau!\" << endl; \n",
      "  }}; \n",
      "int main() { \n",
      "  Animal *animal = new Dog(); \n",
      "  animal->sound();   Output: Gau gau! (Đa hình runtime) \n",
      "  delete animal; \n",
      "  return 0;} \n",
      " \n",
      "4. Thư Viện STL (Standard Template Library) \n",
      "vector: Mảng động, tự quản lý bộ nhớ. \n",
      "#include <vector> \n",
      "vector<int> numbers = {3, 1, 4}; \n",
      "numbers.push_back(5);   Thêm phần tử \n",
      "cout << numbers[2];     Output: 4 \n",
      "algorithm: Cung cấp hàm tiện ích như sort(), find(). \n",
      "#include <algorithm>sort(numbers.begin(), numbers.end());   Sắp xếp tăng dần \n",
      " \n",
      "Phần 2: Thực hành \n",
      " \n",
      "Bài 1: Xây Dựng Lớp Student Quản Lý Điểm \n",
      "Yêu cầu: \n",
      "Thuộc tính: studentId, name, grades (vector<float>). \n",
      "Phương thức: Thêm điểm, tính điểm trung bình. \n",
      "Code mẫu: \n",
      "class Student {private: \n",
      "  string name; \n",
      "  string id; \n",
      "  vector<float> grades;public: \n",
      "  Student(string n, string i) : name(n), id(i) {} \n",
      " \n",
      "  void addGrade(float grade) { \n",
      "    grades.push_back(grade); \n",
      "  } \n",
      " \n",
      "  float calculateAverage() { \n",
      "    float sum = 0; \n",
      "    for (float g : grades) sum += g; \n",
      "    return sum / grades.size(); \n",
      "  } \n",
      " \n",
      "  void displayInfo() { \n",
      "    cout << \"ID: \" << id << \", Ten: \" << name  \n",
      "         << \", Diem TB: \" << calculateAverage() << endl; \n",
      "  }}; \n",
      "int main() { \n",
      "  Student s(\"Alice\", \"SV001\"); \n",
      "  s.addGrade(8.5); \n",
      "  s.addGrade(7.0); \n",
      "  s.displayInfo();   Output: ID: SV001, Ten: Alice, Diem TB: 7.75 \n",
      "  return 0;} \n",
      " \n",
      "Bài 2: Sắp Xếp Danh Sách Sinh Viên Bằng STL \n",
      "Yêu cầu: Sắp xếp danh sách sinh viên theo điểm trung bình giảm dần. \n",
      "Code mẫu: \n",
      "bool compareStudents(const Student &a, const Student &b) { \n",
      "  return a.calculateAverage() > b.calculateAverage();} \n",
      "int main() { \n",
      "  vector<Student> students; \n",
      "  students.push_back(Student(\"Bob\", \"SV002\")); \n",
      "  students.push_back(Student(\"Alice\", \"SV001\")); \n",
      " \n",
      "   Thêm điểm cho từng sinh viên \n",
      "  students[0].addGrade(9.0); \n",
      "  students[1].addGrade(8.5); \n",
      " \n",
      "   Sắp xếp bằng hàm compareStudents \n",
      "  sort(students.begin(), students.end(), compareStudents); \n",
      " \n",
      "   In danh sách đã sắp xếp \n",
      "  for (Student &s : students) { \n",
      "    s.displayInfo(); \n",
      "  } \n",
      "  return 0;} \n",
      " \n",
      " \n",
      "Bài 3: Ứng Dụng STL Khác \n",
      "Tìm kiếm phần tử trong vector: \n",
      "vector<int> nums = {5, 3, 7, 1};auto it = find(nums.begin(), nums.end(), 7);if \n",
      "(it != nums.end()) { \n",
      "  cout << \"Tim thay tai vi tri: \" << it - nums.begin();   Output: 2} \n",
      " \n",
      "Lỗi Thường Gặp & Cách Khắc Phục \n",
      "Quên public khi kế thừa: \n",
      "class Student : Person { ... };   Mặc định là private → Lỗi!class Student : public \n",
      "Person { ... };   Đúng \n",
      "Không khởi tạo vector: Truy cập phần tử khi vector rỗng gây lỗi. \n",
      "Sai phạm vi truy cập: Truy cập thuộc tính private từ bên ngoài lớp. \n",
      " \n",
      "Tips Tối Ưu Hoá \n",
      "Dùng STL thay tự implement: Tiết kiệm thời gian (ví dụ: vector thay mảng \n",
      "động). \n",
      "Virtual destructor: Luôn khai báo destructor ảo trong lớp cơ sở nếu có đa hình. \n",
      "Range-based for loop: Duyệt STL container dễ dàng. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import fitz\n",
    "\n",
    "def extract_text_from_pdf(data_path=\"test.pdf\") -> str:\n",
    "\n",
    "    with fitz.open(data_path) as doc:\n",
    "        text = \"\".join(page.get_text() for page in doc)\n",
    "\n",
    "    return text\n",
    "\n",
    "print(extract_text_from_pdf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d077e6f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116cac15594545aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-27T08:40:26.501241Z",
     "start_time": "2025-06-27T08:40:26.484990Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'llm_output': 'This is the generated output.'}\n"
     ]
    }
   ],
   "source": [
    "from typing import Literal, TypedDict\n",
    "import uuid\n",
    "\n",
    "from langgraph.constants import START, END\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.types import interrupt, Command\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# Define the shared graph state\n",
    "class State(TypedDict):\n",
    "    llm_output: str\n",
    "    decision: str\n",
    "\n",
    "# Simulate an LLM output node\n",
    "def generate_llm_output(state: State) -> State:\n",
    "    return {\"llm_output\": \"This is the generated output.\"}\n",
    "\n",
    "# Human approval node\n",
    "def human_approval(state: State) -> Command[Literal[\"approved_path\", \"rejected_path\"]]:\n",
    "    decision = interrupt({\n",
    "        \"question\": \"Do you approve the following output?\",\n",
    "        \"llm_output\": state[\"llm_output\"]\n",
    "    })\n",
    "\n",
    "    if decision == \"approve\":\n",
    "        return Command(goto=\"approved_path\", update={\"decision\": \"approved\"})\n",
    "    else:\n",
    "        return Command(goto=\"rejected_path\", update={\"decision\": \"rejected\"})\n",
    "\n",
    "# Next steps after approval\n",
    "def approved_node(state: State) -> State:\n",
    "    print(\"✅ Approved path taken.\")\n",
    "    return state\n",
    "\n",
    "# Alternative path after rejection\n",
    "def rejected_node(state: State) -> State:\n",
    "    print(\"❌ Rejected path taken.\")\n",
    "    return state\n",
    "\n",
    "# Build the graph\n",
    "builder = StateGraph(State)\n",
    "builder.add_node(\"generate_llm_output\", generate_llm_output)\n",
    "builder.add_node(\"human_approval\", human_approval)\n",
    "builder.add_node(\"approved_path\", approved_node)\n",
    "builder.add_node(\"rejected_path\", rejected_node)\n",
    "\n",
    "builder.set_entry_point(\"generate_llm_output\")\n",
    "builder.add_edge(\"generate_llm_output\", \"human_approval\")\n",
    "builder.add_edge(\"approved_path\", END)\n",
    "builder.add_edge(\"rejected_path\", END)\n",
    "\n",
    "checkpointer = MemorySaver()\n",
    "graph = builder.compile(checkpointer=checkpointer)\n",
    "\n",
    "# Run until interrupt\n",
    "config = {\"configurable\": {\"thread_id\": uuid.uuid4()}}\n",
    "result = graph.invoke({}, config=config)\n",
    "print(result[\"__interrupt__\"])\n",
    "# Output:\n",
    "# Interrupt(value={'question': 'Do you approve the following output?', 'llm_output': 'This is the generated output.'}, ...)\n",
    "\n",
    "# Simulate resuming with human input\n",
    "# To test rejection, replace resume=\"approve\" with resume=\"reject\"\n",
    "final_result = graph.invoke(Command(resume=\"approve\"), config=config, interrupt_before=[\"human_approval\"])\n",
    "print(final_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a11b52ddfdf60c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-27T09:42:07.398847Z",
     "start_time": "2025-06-27T09:40:56.493679Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWwAAAEICAIAAAD0mFmiAAAAAXNSR0IArs4c6QAAH6hJREFUeJzt3XdcE/f/B/BP9iRsQabiQgRFiy0qooCCVXEgRcRdR62jLly1jmqtdVVbtWrrqqta3FvrnqioIFTrQgRF2SMhBLJ+f6RfftQGgR7kEng9H/5xuc/ncu+7hJefXC53DK1WSwAA/ism3QUAgGlDiAAAJQgRAKAEIQIAlCBEAIAShAgAUMKueld5oeZNSrE0T1ki19RmSSaAL2Za2/OcmgoIg+5SquBtiiLnTWmRVKVV013Kv/BFTAtbrnNzIZNFdylVkPWqJDu9RC5Vq0rrxYkRAjOWTUOuQxPB+7sxqnieyJ83C58lFDFZxM5VWKowvjejYWm1JDO1uFSh6T26oZllNYLY8E5sfaNRE56QJbHkqFRG99Znc5hvU+SKInWXMNuGbny6y3mfS/uz5FINk0Us7fjKknrxJ6CQq2W5SrVa03ecI5tT4X+YVQqR5w+Kkm4UBg5qWNNFmjZZvurGkYzuQ+wkVkaaI4c3pDdpI2nUSkx3IZVQq7QX9qR37GNj78qjuxb9LsZk8QRsr86WdBdCg4yXxQmXcvt97sCqIEcqPyby5oXi7vk8JMi/iS3YnQfYx6xJo7sQ/c7szGjsaWb8CUIIYbEZ3Yc5Htn4ulRhjJ+Ub53JZbFZ9TNBCCF2roLWXawOb0qvqEPlIRJ/ucCrs1VNF1ZHCMQsF3fRo9tSugt5lzRXnfmqxK21Gd2FVEPrzlb3L+XTXcW7tFry5/WC1vU1QXTsGwl0B9f0tlYeIlmvFJYNuLVQWB1hbs3LTi+hu4p35b4tMbfm0F1F9Vg24GamGd2eLMhWcgWsikby9Ye5FTc7vVRvU+UhUlSo4gtN4dA5TfhiZlGBiu4q3lVUqOKZ2qvGE7GKCo1xTwpEJrYnawNPxJJL9b86OE8EAChBiAAAJQgRAKAEIQIAlCBEAIAShAgAUIIQAQBKECIAQAlCBAAoQYgAACUIEQCgBCECAJQgRKASnwz8ePOW9XRXAcbL2EMkOflZZFTv/7DgwUP7li5bUAsVQZUs/HrWyVNH6K4CDMHYQ+TRX0n/bcG/Hv9Z07VANWD/1x/GcnHQgsKCX3/dFBt7raAwv0Vzj+7de37co8/mLet379lGCAkI8hn/+dRPwgffvHn1wsUzCQ/uyWTSlu6eQ4eM9vb+gBDy9NnjsZ8NXrpkzcrvv7GwsBQIhElJCYSQs2dPbNq4q3kzd7q3zwSo1ep9v+/csfMXBoPh0dJr5Ihxnp5tdE1sNufgwb0bNq3h8Xient5zZi8yl5gTQl68eH702P67925nZr51dWkcGjqgd6/+KpWqe4gvIWTFysUbNq4+duQS3VtmaPPmR3M4HC+vths2rmaz2e4tWs2aufD4iYO7dm+1tLQKCe49dswkBoOhGy/Hxl599CiJy+O19fYZNWpCQ3sHQsiBA7/t2bt90cIVy1cuSk1NcXNrGhE+JCSkNyFEJpPF7N91+/aNlJfJVlY2fp26jhwxjs/n617BH9cuv3b9EpfDDQ7u1dLdc87cKYcO/GFhYalSqX7ZvC721rWsrAwvr7b9+0b4+vrV1PYay0hk5crF9+Pjpk79cuvm393dW636fsnDR0mjR02IHDjMzs7+4vm4T8IHy+Xyb76dq1Kpvl64YtuWGEdH57nzpubn5xFCuBwuIWTz1vUDI4ZOn/bV2h+2tGzpGRzc6+L5OCRIFW36+cdjxw4sXrTqqy+X2Ng2mP3lF69epeqaLl46WyQvWr5s3Yzo+UlJ8du2bdDNX7tuRdzdW9OmfLl3z/GePfut+n7JnbhYNpt9+uR1QsiM6Hn1MEEIIVwu907czZSU5zG/n16/dntiUvzkqaPZbM7J41fnzF60d9+OuLu3CCHx8XfXrlvh5dV248Zd3y5Zk5mV8e3Sebpn4HC5Umnh2nUrZs1YcOHcnc5+gStWLc7KyiSE7D+wZ89v2yMjh+/ZdXTShOjzF07v2r1Ft9S+33eeOHl48hezNm7cxWKxN29dTwhhsliEkNVrlh48tHdA2KDf9hz37xy44OuZV65eqKntNZaRSMKDe4Mih7f38SWEjB0zyd8/yNLi3Qu7CoXCzb/sFQqE5uYWhJCxY744dvxgUlKCn19XFotFCOnUscsn4YNp2gLTlp+fF7N/95TJs3UvwUcfdZIXFWVnZzk5uRBCxGKzoUNG6Xpev3H5QeJ93fSCBcuK5XJ7+4aEkL59wk+cOHT79g3dM9RnTCaTzeZMnBDN4XDMJeZujZuqNerhw8YQQtr7+IpF4ufPn7T38fXy8t66eZ+LSyPduzfikyHz5kfLZDKxWMxkMpVK5YTx0z08vAghwcG9ft3x85Mnj2xtG0QOHBbQtbura2NCiK+vX9cu3e/cuTl61ARCyJmzx/07B/p3DiSEDBs6Ou5urK4ehUJx9o8TUYNG9AkdQAjp1bNfUlLCrl1bdD2pM5YQ8fLy3vf7zsLCgo8+7OTp2ca9hYfebvKios2b1yU8uJeTk62bk1+QV9bavFlLQ9Vb1yS/eEYIadnSU/eQzWYvXrSyrNXL07ts2sxMUlry95VQtRpNzIHdt2/fKBuz6N7c4OzsyuH8fY1bgVBobWVT1iQSi2UyKSGExWK9fp22/qdVDx8lFhcX61rz83PF4r8v0O/u3ko3IRabEUJ0S3E4nNt3bny3fOGzZ49VKhUhxMbGlhCiUqlSU1P6hIaXraizX0BiYjwh5K+//lSpVO19OpQ1tfX2OX3mWFFRkUgkor6xxhIis2YuPHp0//kLp/fu2yEWicPCIocOGc1m/6O8t2/fTJ46ur1Ph3lzv/Xw8NJoND16dirfgcsz0ruWGD/dG1QoEOptfeeF0FGr1bNmT9JqtWPHTPL29jETm42fOKL2KzUNTCbzPQ91rly9sGDhzGFDR4/7bEqTJs1u3bo+Z+6U8h10x03e8dPG1X/8cXLsmEntfTrY2dlv+vnHc+dPEUKK5EWEEIHg/+9WZ2lprZuQFUkJIZMmj3rnqXJzs+tUiEjMJEMGfzo4amRSUsKVqxd27NwsMTMfMGBQ+T4XLp5RKpWzZi7UHUYqG4wAdSKRmBAilVXj3hePHz988vSvVSs3tGvbXjdHVp3F4cSJQ61btx05YpzuoaxIVukiGo3m5MnDEZ8M6d2r/99L/W+fC/gCXbKXdc7Ly9FNWFnZEEKmT5vr6Ohc/tlsbBrUyIYYxYHVgoL8g4f2lZSUMBgMLy/vCeOntW7d9vHTR//uZmYm0SUIIeTylfN0FFs3NWvmzmKxEhLu6h5qtdrZX04+c+b4exYpKMgnhNhY2+oeJic/S0t7aZBi64jCwoKyvUcIuXbtYqWLlJaWKhQK6/8tVVpaejP2qm6ay+VaW9ukvEwu63z9xmXdhLOzK5fLZbFYbb19dP9cXRo3cnUrP2yhwihChMlibdu2YeGiWX/++SAvL/fs2RNPn/7l2aoNIcTJySUnJ/v69ctpaS+bNmmek5N94uRhlUoVe+t6YuJ9icQ8M/Ot3ud0dHR+/Pjh/fi4vLxcg2+Q6ZGYSYK79zpyJObU6aP34+PWrltx9+6tVv/7ilevRo2bMBiMmP27ZTLZy5cvftrwfXsf37cZbwghPB7P1rbBvXu378fHGXAjTEyTJs3v3rudkHBPpVL9HrNL95kxo4L3sw6fz3d0dD595tjr9FcFBfnLVy5q6+1TWFigUCgIIR07+J8+ffTe/TsajSZm/26ptFC3lJnYbMTwz7b/uikxMb60tPTS5XMzZk344cdlNbUhRhEiZmKzbxZ/n5WVMfGLT8PCg/fF7Jw4ITq0dxghxPcjPy9P76/mTz9/4Uy3bh8Pjhq5bfvG7iG+hw7vmzRxRnD3Xjt3bdG7O0J7hWm12ugZ458nP6Vjm0zP5C9meXv7rPp+ybTp4xIT4xd/vdLpn6PfdzS0d5j75TeJSfGhfbt+NX/6qFET+vQJT0pK+HT0QELI4KhP4+7emjd/ugG3wMSMGT3xg3YffvnVlOAeHXJysmfOWODewiN6xvhLl8+9Z6n585ZyOJwRI8OHDO3X/gPfTz8dz+Vw+/QLyMzMGDlinKen9/Toz4cND0tLe6n7plJ39sOgyOHR0+ft2bs9tG/XH9cud3RwnhE9v6Y2pPIbem+Y+XzQTDfcAawiL5Kk6c+Kegy3p7uQf3gYW5j2TNExtGY+9BpGdnrJrZOZkdPfl1yG9/p5ceyJ3ODhjnQXUjmFQpGZ+dbFpZHu4d59O/bu23H44PsiqeriL+Xy+OTDED131DWKkQgAULfnt21jxw0+fCSmoCD/wsWzv8fs0p0YUttq+NuZS5fPrVr1jd4mC0ur/AoOT/TpEz5m9MSaraTMvPnR8RV8MleqVBx9X14SQjZu3OXo4FRLJQHUhpEjxhUU5J86dWTjpjW2tnb9+w0cHDXSAOut4RD5sH3Hn3/eo7dJoVCUfbHyDqGwBr6srsiUybNLlfpvRCyVSs3MzPQ22dbQt18ABsNgMKZOmWP49dZwiAiFQqFQ/wlLdLG2tqmoqaFxHccAMEk4JgIAlCBEAIAShAgAUIIQAQBKECIAQAlCBAAoQYgAACUIEQCgBCECAJRUHiLmNhxlicYgxZgklZKIzI3lAnFl+EKWVl2FfsZEWaqRWHHoruJdPAGrkt+51w8atVYgZultqjxExObsnDcltVBVHZH9utjKjkt3Fe+yceRmpMrprqJ6cl4rzK2NLo5tHLgZqcXaev/faGZasbW9/vd55SHi5WfxNL6wFqqqC7Ra8iJJ5uErobuQd0msOVb2vDfJxXQXUg3PE6SenSzorkIPr04WT+4W0F0FnQqylWqV1qGJ/sspVh4ijVsJXZoLrh/JrIXaTJtGTc7tTu8/wVHfRbnp12tUw/hLOdmvTWMUeXHfm059bIxwJEII8e9v8yZZnpxY+YWU6yRpnvLm8cy+nzlU1KHyK5vpxJ7KzctUcnlMW2eBWlXfx3ZqlTbrlSL9ubzPWAdbJ+O9T4WqVHtg3Ssre75AzDKz4mrURvfCMRiMzLTi/CxluwDzpm3EdJfzPsc3vxGasdlcpkUDXj35EyiWqaV5yrcp8k8mOwvN9B8QqUaIEEKyXpWmJ8uLCtVFhXQesnvx4oW5ubmVlZ7LtBmMSMKysuO2+MCMYQrfbj1PKMpKL1FI1UqV0R0iFElY5tacxq1EQkmF71Hj8eJPeXa6oliqKVHUixARSVg2jrzmbSsJ92qEiJGYO3euv79/SEgI3YUAAMF5IgBAFUIEAChBiAAAJQgRAKAEIQIAlCBEAIAShAgAUIIQAQBKECIAQAlCBAAoQYgAACUIEQCgBCECAJQgRACAEoQIAFCCEAEAShAiAEAJQgQAKEGIAAAlCBEAoAQhAgCUIEQAgBKECABQYnohIhAImEzTKxugrjK9v8bi4mKNpl7cfwzAJJheiACAUUGIAAAlCBEAoAQhAgCUIEQAgBKECABQghABAEoQIgBACUIEAChBiAAAJQgRAKAEIQIAlCBEAIAShAgAUIIQAQBKGFqtlu4aqqRt27YMBoPBYBBCdDUzGAxbW9vTp0/TXRpAvWYyI5EWLVrogoPBYDCZTCaTyWAwQkND6a4LoL4zmRAZNmyYQCAoP8fV1TUiIoK+igCAmFKI9OzZ08XFpewhg8EIDAy0tbWltSgAMJ0QIYQMHTqUx+PppjEMATASphQiPXv2bNy4sW4Y0qVLFwxDAIyBKYUIISQqKkooFGIYAmA82JX2yM9S5aSXFEmVBqmnEk6Sjm0ahzZp0iTjKS/jaT7d5RA2h2lmwbZqyBVJKt+TAHVSJeeJnNz6Jj9bJbHm8EUsA1ZlMrh8VvarYiab4dJc0C7Qku5yAGhQcYhoyYF1r1v4mLt6iA1dlAm6cSzT1oHXLtCc7kIADK3CYyJHf0n38LVEglRRx9AGb1MVD28V0l0IgKHpD5E3L0oIg+HUXGjwekxY+2CbxOsFJvIrAoAaoz9Ect4oBCIcKawevogly1cVy9R0FwJgUPpDpFiqFlkgRKrNwpYny1fRXQWAQekPEY2GaFQYl1ebshTDEKh3TOxkMwAwNggRAKAEIQIAlCBEAIAShAgAUIIQAQBKECIAQAlCBAAoQYgAACUIEQCgBCECAJTU2RAJ7dt1955tdFcBUPfV2RCJHDjcy9Ob7ioA6r46+3v/wVEj6S4BoF6osZFISkrywq9n9e0fFBYePG9+dFJSgm5+cI8Oe/ftKOu2dNmC8RNHEEIePkoKCPK5cvXCp6MHBgT5hEf02LBxTVm37OysRYvnDBzUq0+/wCVL56WlvdTNf/rscUCQT2zstfCIHqPHDho/ccTsLyeXL2PO3ClfTBld/uOMVquN2b97zNioj3v5jft86C+b16nVf/9g/3583OSpY3qF+vftHzR56pgbN67o5u8/sCc8ose165eCun+4dv3KmtpFAHVSzYRIaWnptOhxarV69apNy75by2Qy586bVlJS8p5FeFweIWT37q3ffrPm9Mnr4z+fdujwvpOnjhBCVCrVtOhxiUnx0dPnbd8aI5GYT5g4Iv3Na0IIl8MlhGzeun5gxNDp074K6Nr97t1bRUVFuudUKBRxcbGBASHlV3Tw4N6t2zaED4javfNI795hJ04ejtm/mxDyOv3VtOnjnJ1cN/+yd/3abRbmlgu+npmdnUUI4XC4xcXyvft2zJm9qH9f3OAG4H1qJkTS0l7m5eUOGjTCza1ps6Yt5s9bunDBMpXqfdf4YjAYhBB//yB7+4Y8Hi8wILh9+w4XLpwhhCQ8uJeW9nLO7EXtfXytrKwnjp9uJjE/eHAvIYTFYhFCOnXs8kn44JburQIDQlQq1Y0bl3XPee36JY1GExAQXH5FCQ/utWnzQUhIbysr6969+q9bu629TwdCyNGj+21tG0yZPLuhvYOTk8uM6PksFuvsHyd0a5HL5aM+Hd8tqIeTk0sFWwAApMZCxMnJxcLCctnyhQcO/PbX44csFqutt49IJKp0wSZuzcqmHR2ck188I4QkJsZzOJx2bdvr5jMYDO82HyQm3i/r2bxZS92EtbVN69Ztr167qHt4/fql9u07mEv+cd8GT882cXGxy1csunb9klQmdXJ0btKkGSHkZeqLFs092Oy/jwqJxWIX50bJyU/LFmzR3IPaXgGoF2rmwCqPx/th9S8nTh7euXtLQUG+o6PziOGfdQvqUemCfL6g3DS/uFhOCJHJpEqlMiDIp3xPa2ubsmnu/27rTQjp2qX7pp9/UCgULBbrZuzVqZPnvLOKAWGDBALhjZtX5s2PZrPZgYEhY0dPsra2yc3JdnFp9I9iBAJ5sfz/18LlVnM3ANRHNfbtjItLo8/HTRk5YlxcXOzps8eWfPtVI1e3pk2bv9NNo/7HVUhlMmnZtEKhEAiEurwQCARLvln9j0JZ+kvt2qXbuvUrY29dY7PZWq3W3z/onQ4sFiu0d1ho77CUlOS7d29t/3WTvKho8aKVQpFIUaIo37NYLnd1afxfdwBAPVUzIfLy5YtHfyX1CAnl8/l+fl19ff1CPu74+MnDpk2b83i84nL/vaemprDY/7/S+IS7fn5dddPPnj12a9yUEOLm1qy4uNje3qGhvYOu6XX6KytLa72rtrS0+qDdh3fu3JRKC/06dRUIBOVbtVrt2bMnWrTwaNTITfevUFpw5uxx3aeVP86dVKlUuk80hdLCl6kvevToUyM7BKD+qJljIvn5ecuWf71h45rX6a9SUpJ379mm0WhaebQmhLRq1ebqtYu6L1B27tqSk5tdfsE7cTfvxMUSQi5fOX8/Pi4wMIQQ8tGHHT/8sOOKFYsyMt4WFOQfPLTv8/HDTp0+WtHau3TplpBw99792wFdg99pYjAYZ84eX/D1zJs3rxZKC2Njr127fklXWO9e/aXSwu9Xf5uR8TYlJXnpd/MFAuHHCBGAaqqZkUibNu2mTf1y+6+bfo/ZRQhp7+O7etWmRo3cCCGTJs5Yteqb3n26sNnsgRFDuwV9fP/+nbIFoyJHbNy0ZuasZywWa0DYoJ4f99XNX7pkzdFjBxZ9M+fhw0RnZ9ceIaFh/QdWtPauXbp/v/pbHo/n6+v379ZZMxeuW7/yy6+m6j4o9e7V/5PwIYQQZ2fXBfO/27lzc2RUbwsLy5YtPdf+sEUoxE3/AKpH/w29b53KVSpJmy5Wtbfi5ORno8ZE/rD6l9at29beWgzsxOa0wIgGDZx5VegLUEfU2d/OAIBhIEQAgBLafoDn5tb04vk4utYOADUFIxEAoAQhAgCUIEQAgBKECABQghABAEoQIgBACUIEAChBiAAAJQgRAKAEIQIAlOgPEb6ISRgGr8X08QQsLh+5DPWL/ne8pR03K1WhtwkqoirVZrwstrDl0F0IgEHpDxHnZkKFXF0i1xi8HhOW8lDWqoN5FToC1Cn6Q4TBJMFD7S7FvFGr9FyyCP7t1RN58oPCzv1sqtAXoE7Rf2UznbxM5W8rUj06WFjYcPkilmELMw0sFiMvs6S0WJORKg+b4MTA8RCof94XIjoJV/KzXpcWFbzvdnaGlJmZKRKKROLK74xlAGJzNofPsHPmt/Axo7sWAHpUHiLGZu7cuf7+/iEhIVXoCwC1DuNvAKAEIQIAlCBEAIAShAgAUIIQAQBKECIAQAlCBAAoQYgAACUIEQCgBCECAJQgRACAEoQIAFCCEAEAShAiAEAJQgQAKEGIAAAlCBEAoAQhAgCUIEQAgBKECABQghABAEoQIgBACUIEACgxvRCRSCQcDm6aDWAsTC9ECgsLlUol3VUAwN9ML0QAwKggRACAEoQIAFCCEAEAShAiAEAJQgQAKEGIAAAlCBEAoAQhAgCUIEQAgBKECABQghABAEoQIgBACUIEAChBiAAAJQytVkt3DVXSvXt3LpfLZDJzc3OFQqFums1mHzp0iO7SAOo1Nt0FVJWlpWVycrJuuqSkRDcxcOBAWosCANP5OBMVFcXj8crPcXR0jIyMpK8iACCmFCL9+vVzdHQsP6dTp07Ozs70VQQAxJRChBASGRlZNhhxdnaOioqiuyIAMKkQCQsLKxt6dOjQwcnJie6KAMCkQoQQEhERweVynZyccDQEwEj8l29n5FJ1TnpJiUJTC/VUonWTYA/Xe+7u7qV5Vs/yZAZeO4PBEElYVvZcLt/Ewheg9lTvPBFlifbs7oy3KcXOLUSlxTSECL2YbIYsT6mQq5u0FncKtaa7HACjUI0QUcg1B9e96tDLzsaJV4XuddmDK3kKuTJoYAO6CwGgXzWG5XtXpgZGOiBBCCGt/S0FYs7lg1l0FwJAv6qGyINrBc3bmYvMTeYM19rm5WeZl6HMz8INPaG+q2qIvH2pEEqQIP/AYjNy3pTSXQUAzaoaIkqFRmLNreViTIxFA64sHyMRqO+qGiKKIrVGXe++jnk/lZKo1abxG2iA2oPzHQCAEoQIAFCCEAEAShAiAEAJQgQAKEGIAAAlCBEAoAQhAgCUIEQAgBKECABQghABAErqYIicO386IMinUFpIdyEA9UIdDBEAMCSECABQYnTXGUpMjP91x8+PHz+0srbx/chv2NAxIpGIEHLgwG979m5ftHDF8pWLUlNT3NyaRoQPCQnprVtq46Yfzv5xQigQBgX1cHTAbfEADMe4RiKpqSkzZ09UqpTr121fMO+7p0//mh49TqPREEI4XK5UWrh23YpZMxZcOHens1/gilWLs7IyCSFHju4/cjRm8hezfvpph51dw527t9C9HQD1iHGFyLnzpzhszqKFK1xcGrm5NZ0xY/7jJ49u3LxCCGEymUqlcsL46R4eXgwGIzi4l1qtfvLkESHk4KG9Xfy7dfEPkphJen7ct03rdnRvB0A9YlwhkpSU4O7eytzcQvewob2Dg4NTQsK9sg7u7q10E2KxGSFEJpNqtdrXr9MaNXIr69OihYfBCweov4zrmIhMJn367HFAkE/5mXl5OWXTDAbjnUWKiorUarVIJC6bw+fxa79SAPibcYWIlbWNl0AwcsS48jPNJRbvWUQkErFYrNKSkrI58mJ5bdYIAP9gXCHSxK3ZxYtnvdt8UDbiSElJdnJyec8iDAbDzq7hnw8fDBgwSDcn9tY1gxQLAMTojolERAxVqVXrflqlUChSU1M2bvrh09EDX6Q8f/9SAV27X7z0x+Ur5wkhe37b/vjxQ0PVCwBGFiLmEvMtm/fxefzPPh8yfGR4woN7s2YsaNa0xfuXGjJ4VI+Q0B9+XBYQ5BN769rnn00hhGg1uMEFgCFU9YbeB3581aartZ2roPZLMhlxf+SYWzPbBVjSXQgAnYxrJAIAJqdWDqy+fftmzNhBepuYLJZGrdbb1KdP+JjRE2uwjH5h3dQq1b/nq9QqQgibpWfbO/l1nT1zYQ3WAFDn1UqI2NjY/vzzHr1NMqlUbGamt0koFNVsGRt+2lFRU0lJCY/H+/d8AR+f1wCqp1ZChM1mN7R30N9mXxsr1K/CGgCg5uCYCABQghABAEoQIgBACUIEAChBiAAAJQgRAKAEIQIAlCBEAIAShAgAUFLVEJHYcLTady9NWM+xOQyBkEV3FQA0q2qIiCTsrFfFtVyMiUl/Lre059JdBQDNqhoibp7ivIySKnSsL0rkGjabYe+Ci0JDfVfVELFvxHNw4984mlnL9ZiMC/vSu4TbEnzCg3qvqlc200m4kv/6ucLWWWDjwGfWv6MBDMIokqpkuaV3zmYPmuFiaYfPMgDVDBFCyOvnimfx0mKZOj+rtNaqMlIsNpMvYtq58H26WdXDDAXQq9ohAgBQHs4TAQBKECIAQAlCBAAoQYgAACUIEQCgBCECAJQgRACAkv8DkYwyywyuMpEAAAAASUVORK5CYII=",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "from typing import Any, Coroutine\n",
    "\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph_swarm import create_swarm\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import StateGraph\n",
    "from src.agents.manage.manage import agent as agent1\n",
    "from src.agents.chat.chat import agent as agent2\n",
    "from src.agents.state import State\n",
    "from src.config.setup import GOOGLE_API_KEY\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langgraph.types import interrupt, Command\n",
    "\n",
    "model = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    google_api_key=GOOGLE_API_KEY,\n",
    "    disable_streaming=False,\n",
    ")\n",
    "\n",
    "\n",
    "async def supervisor(state: State):\n",
    "    resp = await model.ainvoke(state[\"messages\"] + [SystemMessage(\n",
    "        content=\"\"\"You are an expert router, reading the user content and returning a word: \"chat\" if it's a normal chat request, or \"manage\" if it's a task management request like checking orders, processing requests from the system.\"\"\")])\n",
    "    print(resp)\n",
    "    resp = resp.content.strip()\n",
    "\n",
    "    if \"chat\" == resp:\n",
    "        return Command(goto=\"chat\",\n",
    "                       update={\"active_agent\": \"chat\"}, )\n",
    "    elif \"manage\" == resp:\n",
    "        return Command(goto=\"manage\",\n",
    "                       update={\"active_agent\": \"manage\"}, )\n",
    "\n",
    "    return Command(goto=\"chat\", update={\"active_agent\": \"chat\"})\n",
    "\n",
    "\n",
    "checkpointer = MemorySaver()\n",
    "\n",
    "workflow = StateGraph(State)\n",
    "workflow.add_node(\"chat\", agent1)\n",
    "workflow.add_node(\"manage\", agent2)\n",
    "workflow.add_node(\"supervisor\", supervisor)\n",
    "workflow.set_entry_point(\"supervisor\")\n",
    "\n",
    "swarm = workflow.compile(checkpointer=checkpointer)\n",
    "\n",
    "display(Image(swarm.get_graph().draw_mermaid_png()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83caa1652e4cd0c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-27T08:59:34.342469Z",
     "start_time": "2025-06-27T08:59:34.316900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entered `parent_node` a total of 1 times\n",
      "Entered `node_in_subgraph` a total of 1 times\n",
      "Entered human_node in sub-graph a total of 1 times\n",
      "{'__interrupt__': (Interrupt(value='what is your name?', resumable=True, ns=['parent_node:c82e3fef-1cd5-e3c6-aad7-dba5000d3fa4', 'human_node:1cd4b043-1782-3e7d-6d89-32a36e61b9d6']),)}\n",
      "--- Resuming ---\n",
      "Entered `parent_node` a total of 2 times\n",
      "Entered human_node in sub-graph a total of 2 times\n",
      "Got an answer of Lưu Trọng Dũng\n",
      "{'parent_node': {'state_counter': 1}}\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "from typing import TypedDict\n",
    "\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.constants import START\n",
    "from langgraph.types import interrupt, Command\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    \"\"\"The graph state.\"\"\"\n",
    "    state_counter: int\n",
    "\n",
    "\n",
    "counter_node_in_subgraph = 0\n",
    "\n",
    "def node_in_subgraph(state: State):\n",
    "    \"\"\"A node in the sub-graph.\"\"\"\n",
    "    global counter_node_in_subgraph\n",
    "    counter_node_in_subgraph += 1  # This code will **NOT** run again!\n",
    "    print(f\"Entered `node_in_subgraph` a total of {counter_node_in_subgraph} times\")\n",
    "\n",
    "counter_human_node = 0\n",
    "\n",
    "def human_node(state: State):\n",
    "    global counter_human_node\n",
    "    counter_human_node += 1 # This code will run again!\n",
    "    print(f\"Entered human_node in sub-graph a total of {counter_human_node} times\")\n",
    "    answer = interrupt(\"what is your name?\")\n",
    "    print(f\"Got an answer of {answer}\")\n",
    "\n",
    "\n",
    "checkpointer = MemorySaver()\n",
    "\n",
    "subgraph_builder = StateGraph(State)\n",
    "subgraph_builder.add_node(\"some_node\", node_in_subgraph)\n",
    "subgraph_builder.add_node(\"human_node\", human_node)\n",
    "subgraph_builder.add_edge(START, \"some_node\")\n",
    "subgraph_builder.add_edge(\"some_node\", \"human_node\")\n",
    "subgraph = subgraph_builder.compile(checkpointer=checkpointer)\n",
    "\n",
    "\n",
    "counter_parent_node = 0\n",
    "\n",
    "def parent_node(state: State):\n",
    "    \"\"\"This parent node will invoke the subgraph.\"\"\"\n",
    "    global counter_parent_node\n",
    "\n",
    "    counter_parent_node += 1 # This code will run again on resuming!\n",
    "    print(f\"Entered `parent_node` a total of {counter_parent_node} times\")\n",
    "\n",
    "    # Please note that we're intentionally incrementing the state counter\n",
    "    # in the graph state as well to demonstrate that the subgraph update\n",
    "    # of the same key will not conflict with the parent graph (until\n",
    "    subgraph_state = subgraph.invoke(state)\n",
    "    return subgraph_state\n",
    "\n",
    "\n",
    "builder = StateGraph(State)\n",
    "builder.add_node(\"parent_node\", parent_node)\n",
    "builder.add_edge(START, \"parent_node\")\n",
    "\n",
    "# A checkpointer must be enabled for interrupts to work!\n",
    "checkpointer = MemorySaver()\n",
    "graph = builder.compile(checkpointer=checkpointer)\n",
    "\n",
    "config = {\n",
    "    \"configurable\": {\n",
    "      \"thread_id\": uuid.uuid4(),\n",
    "    }\n",
    "}\n",
    "\n",
    "for chunk in graph.stream({\"state_counter\": 1}, config):\n",
    "    print(chunk)\n",
    "\n",
    "print('--- Resuming ---')\n",
    "\n",
    "for chunk in graph.stream(Command(resume=\"Lưu Trọng Dũng\"), config):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078fdc84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lỗi kết nối: (psycopg.OperationalError) [Errno 11003] getaddrinfo failed\n",
      "(Background on this error at: https://sqlalche.me/e/20/e3q8)\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import create_engine\n",
    "\n",
    "DB_PASSWORD = \"Matkhaula@123\"\n",
    "DATABASE_URL = f\"postgresql+psycopg://postgres.rcvgpqodvckwzzrfbxml:{DB_PASSWORD}@aws-1-ap-southeast-1.pooler.supabase.com:6543/postgres\"\n",
    "\n",
    "engine = create_engine(DATABASE_URL)\n",
    "\n",
    "try:\n",
    "    with engine.connect() as conn:\n",
    "        result = conn.execute(\"SELECT 1\")\n",
    "        print(\"Kết nối thành công:\", result.fetchone())\n",
    "except Exception as e:\n",
    "    print(\"Lỗi kết nối:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a34de8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "from langchain_core.runnables.graph import MermaidDrawMethod\n",
    "from IPython.display import display, Image\n",
    "\n",
    "chat = ChatAgent().get_builder().compile()\n",
    "\n",
    "display(\n",
    "    Image(\n",
    "        chat.get_graph().draw_mermaid_png(\n",
    "            draw_method=MermaidDrawMethod.API,\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a130b7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "    async def process(self, state: State) -> State:\n",
    "        state[\"current_agent\"] = \"analysis\"\n",
    "\n",
    "        current_step = state[\"agent_logs\"][-1][\"step\"] + 1 if state[\"agent_logs\"] else 1\n",
    "        start_time = time()\n",
    "\n",
    "        result_msg = await self._chain.ainvoke({\"messages\": state[\"messages\"]})\n",
    "        result_text = getattr(result_msg, \"content\", str(result_msg))\n",
    "\n",
    "        end_time = time()\n",
    "        duration = end_time - start_time\n",
    "\n",
    "        state[\"agent_logs\"].append(\n",
    "            {\n",
    "                \"agent_name\": \"analysis\",\n",
    "                \"task\": \"Analyze user messages\",\n",
    "                \"result\": result_text,\n",
    "                \"step\": current_step,\n",
    "                \"start_time\": start_time,\n",
    "                \"end_time\": end_time,\n",
    "                \"duration\": duration,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        state[\"messages\"].append({\"role\": \"assistant\", \"content\": result_text})\n",
    "\n",
    "        return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e2e889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analysis\n",
      "\n",
      "\n",
      "supervisor (0.68s): assignment for analysis\n",
      "Here\n",
      "'s a breakdown of how to analyze the user's request:\n",
      "\n",
      "1. **\n",
      "Identify the core question:** The user wants to understand the concept of a derivative.\n",
      "\n",
      "2. **Identify the desired output:** The user wants a definition of derivative and example calculations.\n",
      "3. **Outline the steps to fulfill the request:**\n",
      "\n",
      "    *   Explain the definition of a derivative (conceptually and mathematically).\n",
      "    *   Provide a few example functions.\n",
      "    *   Demonstrate the\n",
      " calculation of the derivative for each example function, showing the steps.\n",
      "\n",
      "analysis (1.45s): solved\n",
      "NO\n",
      "_MATH\n",
      "\n",
      "\n",
      "logic (0.77s): skipped\n",
      "Okay\n",
      ", I can\n",
      " definitely help with that! Let's break down the concept of a derivative and then walk\n",
      " through some examples.\n",
      "\n",
      "**What is a Derivative?**\n",
      "\n",
      "At its heart\n",
      ", the derivative of a function tells you the *instantaneous rate of change* of that function at a specific point.  Think of it like this:\n",
      "\n",
      "*   \n",
      "**Imagine you're driving a car.** Your speed is how quickly your position (distance traveled) is changing with respect to time.  Your speedometer isn't showing\n",
      " your *average* speed over a long period, but your speed at *this very moment*.  The derivative is like that instantaneous speed.\n",
      "\n",
      "*   **Graphically:** If you plot a function on a graph, the derivative at a point is\n",
      " the slope of the line that's tangent to the curve at that point. A tangent line just \"kisses\" the curve at that specific location.\n",
      "\n",
      "**The Mathematical Definition**\n",
      "\n",
      "The formal definition of the derivative of a function *f\n",
      "(x)*, denoted as *f'(x)* (read as \"f prime of x\"), is:\n",
      "\n",
      "f'(x) = lim (h -> 0)  [f(x + h) - f(x)] / h\n",
      "\n",
      "Let's break that down:\n",
      "\n",
      "*   **f(\n",
      "x + h):**  This means you're taking a point *x* and adding a tiny amount *h* to it, then plugging that into your function.\n",
      "*   **f(x + h) - f(x):** This is the *change* in the function's value as you\n",
      " move from *x* to *x + h*.  It's the \"rise\" in the slope calculation.\n",
      "*   **h:** This is the tiny change in *x*. It's the \"run\" in the slope calculation.\n",
      "*   **[f(x + h) - f(x\n",
      ")] / h:** This is the slope of a line connecting the points (x, f(x)) and (x + h, f(x + h)) on the graph of the function.  This is called a *secant line*.\n",
      "*   **lim (h -> 0):** This is the\n",
      " crucial part. We're taking the limit as *h* gets infinitely small (approaches zero).  As *h* shrinks towards zero, that secant line becomes closer and closer to the tangent line at the point *x*.  The limit *is* the slope of the tangent line, and thus the\n",
      " derivative.\n",
      "\n",
      "**Examples and Derivative Calculations**\n",
      "\n",
      "Let's look at a few examples to make this more concrete.  I'll show how to find the derivative using the definition above.  (Note: There are faster rules for finding derivatives once you get the hang of it, but for now, we'll stick\n",
      " with the definition.)\n",
      "\n",
      "**Example 1:  f(x) = x<sup>2</sup>**\n",
      "\n",
      "1.  **Apply the definition:**\n",
      "\n",
      "    f'(x) = lim (h -> 0)  [ (x + h)<sup>2</sup> - x<sup>2</sup> ] / h\n",
      "\n",
      "2.  **\n",
      "Expand (x + h)<sup>2</sup>:**\n",
      "\n",
      "    f'(x) = lim (h -> 0)  [ x<sup>2</sup> + 2xh + h<sup>2</sup> - x<sup>2</sup> ] / h\n",
      "\n",
      "3.  **Simplify:**\n",
      "\n",
      "    f'(x) = lim (h -> \n",
      "0)  [ 2xh + h<sup>2</sup> ] / h\n",
      "\n",
      "4.  **Factor out an h from the numerator:**\n",
      "\n",
      "    f'(x) = lim (h -> 0)  [ h(2x + h) ] / h\n",
      "\n",
      "5.  **Cancel the h's:**\n",
      "\n",
      "\n",
      "    f'(x) = lim (h -> 0)  [ 2x + h ]\n",
      "\n",
      "6.  **Take the limit as h approaches 0:**\n",
      "\n",
      "    f'(x) = 2x + 0 = 2x\n",
      "\n",
      "Therefore, the derivative of f(x) =\n",
      " x<sup>2</sup> is f'(x) = 2x. This means that the slope of the tangent line to the curve *y = x<sup>2</sup>* at any point *x* is *2x*.\n",
      "\n",
      "**Example 2: f(x) = 3x + 1**\n",
      "\n",
      "\n",
      "1.  **Apply the definition:**\n",
      "\n",
      "    f'(x) = lim (h -> 0)  [ 3(x + h) + 1 - (3x + 1) ] / h\n",
      "\n",
      "2.  **Distribute:**\n",
      "\n",
      "    f'(x) = lim (h -> 0)\n",
      "  [ 3x + 3h + 1 - 3x - 1 ] / h\n",
      "\n",
      "3.  **Simplify:**\n",
      "\n",
      "    f'(x) = lim (h -> 0)  [ 3h ] / h\n",
      "\n",
      "4.  **Cancel the h's:**\n",
      "\n",
      "    \n",
      "f'(x) = lim (h -> 0)  [ 3 ]\n",
      "\n",
      "5.  **Take the limit as h approaches 0:**\n",
      "\n",
      "    f'(x) = 3\n",
      "\n",
      "Therefore, the derivative of f(x) = 3x + 1 is f'(x) = \n",
      "3.  This makes sense because *f(x) = 3x + 1* is a straight line with a constant slope of 3.  The derivative *is* the slope, and it's constant for all values of *x*.\n",
      "\n",
      "**Example 3: f(x) = c\n",
      " (where c is a constant)**\n",
      "\n",
      "1.  **Apply the definition:**\n",
      "\n",
      "    f'(x) = lim (h -> 0) [c - c] / h\n",
      "\n",
      "2.  **Simplify:**\n",
      "\n",
      "     f'(x) = lim (h -> 0) 0 / h\n",
      "\n",
      "3.  **\n",
      "Take the limit:**\n",
      "\n",
      "    f'(x) = 0\n",
      "\n",
      "Therefore, the derivative of a constant function is always 0. This is because a constant function doesn't change its value, so its rate of change is zero.\n",
      "\n",
      "**Key Takeaways**\n",
      "\n",
      "*   The derivative is the instantaneous rate of\n",
      " change of a function.\n",
      "*   Geometrically, the derivative is the slope of the tangent line.\n",
      "*   The formal definition involves a limit as *h* approaches zero.\n",
      "*   Finding derivatives using the definition can be a bit tedious, but it's important to understand the underlying concept.\n",
      "\n",
      "I\n",
      " hope this helps!  Do you have any specific functions you'd like me to find the derivative of, or any particular aspect of the derivative that you'd like me to explain in more detail?\n",
      "\n",
      "writer (8.38s): solved\n"
     ]
    }
   ],
   "source": [
    "from typing import Sequence\n",
    "from langchain_core.tools.base import BaseTool\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from typing import Sequence\n",
    "from time import time\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.tools.base import BaseTool\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langgraph.graph import MessagesState\n",
    "from typing import TypedDict\n",
    "from pydantic import Field\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from time import time\n",
    "import json \n",
    "\n",
    "GOOGLE_API_KEY = \"AIzaSyBbZBWB-7R5_uOI4_WM73G-cl3LWUZvsUg\"\n",
    "\n",
    "class AgentLog(TypedDict):\n",
    "    agent_name: str\n",
    "    task: str\n",
    "    result: str\n",
    "    step: int\n",
    "    start_time: float\n",
    "    end_time: float\n",
    "    duration: float\n",
    "\n",
    "\n",
    "class State(MessagesState):\n",
    "    thread_id: str\n",
    "    agent_logs: list[AgentLog] = Field(default_factory=list)\n",
    "    next_agent: str\n",
    "    prev_agent: str\n",
    "    task: str\n",
    "    human: False\n",
    "\n",
    "\n",
    "class BaseAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        agent_name: str,\n",
    "        tools: Sequence[BaseTool] | None = None,\n",
    "        model: object | None = None,\n",
    "    ) -> None:\n",
    "        self._tools = list(tools or [])\n",
    "        self._agent_name = agent_name\n",
    "\n",
    "        self._model = ChatGoogleGenerativeAI(\n",
    "            model=\"gemini-2.0-flash\",\n",
    "            google_api_key=GOOGLE_API_KEY,\n",
    "            disable_streaming=False,\n",
    "        ).bind_tools(self._tools)\n",
    "\n",
    "    async def process(self, state: State) -> State:\n",
    "        return state\n",
    "\n",
    "    def response_filter(self, content: str):\n",
    "        lines = content.strip().splitlines()\n",
    "        last_line = lines[-1].strip()\n",
    "\n",
    "        human_dict = json.loads(last_line.lower())\n",
    "        clean_content = \"\\n\".join(lines[:-1]).strip()\n",
    "        return (clean_content, human_dict[\"human\"])\n",
    "\n",
    "    def human_node(self, state: State) -> State:\n",
    "        if state[\"human\"]:\n",
    "            print(\"++++++++++++++++++++++++++++++++++++\")\n",
    "            interrupt({\"AIMessage\": state[\"task\"]})\n",
    "        else:\n",
    "            return state\n",
    "\n",
    "    def get_graph(self) -> StateGraph:\n",
    "        graph = StateGraph(State)\n",
    "        graph.add_node(self._agent_name, self.process)\n",
    "        graph.add_node(\"human_node\", self.human_node)\n",
    "\n",
    "        if self._tools:\n",
    "            graph.add_node(\"tools\", ToolNode(self._tools))\n",
    "            graph.add_conditional_edges(\n",
    "                self._agent_name,\n",
    "                tools_condition,\n",
    "                {\"tools\": \"tools\", \"__end__\": \"human_node\"},\n",
    "            )\n",
    "            graph.add_edge(\"tools\", self._agent_name)\n",
    "        else:\n",
    "            graph.add_edge(self._agent_name, \"human_node\")\n",
    "\n",
    "        graph.set_entry_point(self._agent_name)\n",
    "        graph.set_finish_point(\"human_node\")\n",
    "        return graph.compile(name=self._agent_name)\n",
    "\n",
    "\n",
    "class AnalysisAgent(BaseAgent):\n",
    "    def __init__(self, tools: Sequence[BaseTool] | None = None) -> None:\n",
    "        super().__init__(\n",
    "            agent_name=\"analysis\",\n",
    "            tools=tools or [],\n",
    "            model=None,\n",
    "        )\n",
    "\n",
    "        self._prompt = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                SystemMessage(\n",
    "                    content=\"\"\"\n",
    "                You are the ANALYSIS agent.\n",
    "                Your task: analyze the user's request only.\n",
    "                Be concise, structured, and focus on reasoning or problem-solving steps.\n",
    "                Do not generate the final answer or solution — only break down the request into what needs to be done.\n",
    "                \"\"\"\n",
    "                ),\n",
    "                MessagesPlaceholder(\"task\"),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self._chain = self._prompt | self._model\n",
    "\n",
    "    async def process(self, state: State) -> State:\n",
    "        start = time()\n",
    "        result_msg = await self._chain.ainvoke(\n",
    "            {\"task\": [HumanMessage(content=state[\"task\"])]}\n",
    "        )\n",
    "        end = time()\n",
    "        duration = end - start\n",
    "        state[\"task\"] = result_msg.content\n",
    "        print(f\"analysis ({duration:.02f}s): solved\")\n",
    "        state[\"next_agent\"] = \"logic\"\n",
    "        state[\"prev_agent\"] = \"analysis\"\n",
    "        return state\n",
    "\n",
    "class WriterAgent(BaseAgent):\n",
    "    def __init__(self, tools: Sequence[BaseTool] | None = None) -> None:\n",
    "        super().__init__(\n",
    "            agent_name=\"writer\",\n",
    "            tools=tools or [],\n",
    "            model=None,\n",
    "        )\n",
    "\n",
    "        self._prompt = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                SystemMessage(content=\"\"\"  \n",
    "                    You are the WRITER agent.\n",
    "                    You should use the results from the previous agent's step and continue processing.\n",
    "                    Your job: respond naturally and helpfully, based on the prior analysis or direct user message.\n",
    "                \"\"\"),\n",
    "                \n",
    "                MessagesPlaceholder(\"task\"),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self._chain = self._prompt | self._model\n",
    "\n",
    "    async def process(self, state: State) -> State:\n",
    "        start = time()\n",
    "        result_msg = None\n",
    "        if state[\"prev_agent\"] != \"supervisor\":\n",
    "            result_msg = await self._chain.ainvoke(\n",
    "                {\"task\": [HumanMessage(content=state[\"task\"])]}\n",
    "            )\n",
    "        elif state[\"prev_agent\"] == \"supervisor\":\n",
    "            result_msg = await self._chain.ainvoke(\n",
    "                {\"task\": [HumanMessage(content=state[\"task\"])]}\n",
    "            )\n",
    "        end = time()\n",
    "        duration = end - start\n",
    "        print(f\"writer ({duration:.02f}s): solved\")\n",
    "        return {\"messages\": [result_msg]}\n",
    "\n",
    "\n",
    "class LogicAgent(BaseAgent):\n",
    "    def __init__(self, tools: Sequence[BaseTool] | None = None) -> None:\n",
    "        super().__init__(\n",
    "            agent_name=\"logic\",\n",
    "            tools=tools or [],\n",
    "            model=None,\n",
    "        )\n",
    "\n",
    "        self._prompt = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                SystemMessage(\n",
    "                    content=\"\"\"\n",
    "                You are the LOGIC/MATH agent.\n",
    "                You should use the results from the previous agent's step and continue processing.\n",
    "                Goal:\n",
    "                - If the user's request clearly involves mathematics (arithmetic, algebra, calculus, probability, statistics, unit conversion, dimensional analysis, etc.), then you must SOLVE it carefully following the rules below.\n",
    "                - If the request DOES NOT contain any mathematical or logical problems, do not attempt to answer. Instead, return NO_MATH.\n",
    "                This tag indicates the supervisor should route the request to another agent.\n",
    "\n",
    "                Rules for solving math:\n",
    "                1) Work carefully and compute digit-by-digit where needed; keep units and significant figures consistent.\n",
    "                2) State minimal assumptions only when data is missing/ambiguous; never invent data.\n",
    "                3) Prefer exact forms (fractions, radicals) unless a decimal is requested; if rounding, specify the rule.\n",
    "                4) Provide a brief structured result:\n",
    "                - Answer: <final value with units if any>\n",
    "                - Justification: 2-4 concise steps (no long chain-of-thought)\n",
    "                - Check: a quick verification (plug back / dimension / sanity)\n",
    "                5) If multiple sub-parts, label (a), (b), (c) clearly.\n",
    "                6) Mirror the user's language (Vietnamese or English).\n",
    "                7) Do not use external tools or the web.\n",
    "                \"\"\"\n",
    "                ),\n",
    "                MessagesPlaceholder(\"task\"),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self._chain = self._prompt | self._model\n",
    "\n",
    "    async def process(self, state: State) -> State:\n",
    "        start = time()\n",
    "        result_msg = await self._chain.ainvoke(\n",
    "            {\"task\": [HumanMessage(content=state[\"task\"])]}\n",
    "        )\n",
    "        end = time()\n",
    "        duration = end - start\n",
    "\n",
    "        if \"NO_MATH\" in result_msg.content:\n",
    "            print(f\"logic ({duration:.02f}s): skipped\")\n",
    "            state[\"task\"] = state[\"task\"]\n",
    "        else:\n",
    "            print(f\"logic ({duration:.02f}s): solved\")\n",
    "            state[\"task\"] = result_msg.content\n",
    "        state[\"next_agent\"] = \"writer\"\n",
    "        state[\"prev_agent\"] = \"logic\"\n",
    "        return state\n",
    "\n",
    "\n",
    "class SupervisorAgent(BaseAgent):\n",
    "    def __init__(self, tools: Sequence[BaseTool] | None = None) -> None:\n",
    "        super().__init__(\n",
    "            agent_name=\"supervisor\",\n",
    "            tools=tools or [],\n",
    "            model=None,\n",
    "        )\n",
    "\n",
    "        self._prompt = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\n",
    "                    \"system\",\n",
    "                    \"You are the SUPERVISOR agent.\\n\"\n",
    "                    \"Your job: decide which specialized agent should handle the user's last message.\\n\\n\"\n",
    "                    \"Rules:\\n\"\n",
    "                    \"- If the request is casual conversation, chit-chat, or simple → respond ONLY with: writer\\n\"\n",
    "                    \"- If the request is complex, requires reasoning, problem-solving, or analysis → respond ONLY with: analysis\\n\\n\"\n",
    "                    \"IMPORTANT: Output exactly one word: 'writer' or 'analysis'.\",\n",
    "                ),\n",
    "                MessagesPlaceholder(\"assignment\"),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self._chain = self._prompt | self._model\n",
    "\n",
    "    async def process(self, state: State) -> State:\n",
    "        start = time()\n",
    "        result_msg = await self._chain.ainvoke({\"assignment\": [state[\"messages\"][-1]]})\n",
    "        end = time()\n",
    "        duration = end - start\n",
    "        state[\"next_agent\"] = result_msg.content.strip().lower()\n",
    "        state[\"task\"] = state[\"messages\"][-1].content\n",
    "        state[\"prev_agent\"] = \"supervisor\"\n",
    "        print(f\"supervisor ({duration:.02f}s): assignment for {state[\"next_agent\"]}\")\n",
    "        return state\n",
    "\n",
    "def route(state: State) -> str:\n",
    "    if state[\"next_agent\"] == \"analysis\":\n",
    "        return \"analysis\"\n",
    "    elif state[\"next_agent\"] == \"writer\":\n",
    "        return \"writer\"\n",
    "\n",
    "\n",
    "app = StateGraph(State)\n",
    "\n",
    "supervisor = SupervisorAgent()\n",
    "analysis = AnalysisAgent()\n",
    "writer = WriterAgent()\n",
    "logic = LogicAgent()\n",
    "\n",
    "app.add_node(\"supervisor\", supervisor.get_builder().compile())\n",
    "app.add_node(\"analysis\", analysis.get_builder().compile())\n",
    "app.add_node(\"writer\", writer.get_builder().compile())\n",
    "app.add_node(\"logic\", logic.get_builder().compile())\n",
    "\n",
    "app.set_entry_point(\"supervisor\")\n",
    "app.add_conditional_edges(\"supervisor\", route, {\"analysis\": \"analysis\", \"writer\":\"writer\"})\n",
    "app.add_edge(\"analysis\", \"logic\")\n",
    "app.add_edge(\"logic\",\"writer\")\n",
    "app.set_finish_point(\"writer\")\n",
    "\n",
    "workflow = app.compile(checkpointer=MemorySaver())\n",
    "\n",
    "input_state = {\n",
    "    \"messages\": [\n",
    "        HumanMessage(content=\"đạo hàm là gi và ví dụ tính toán cụ thể\"),\n",
    "    ],\n",
    "    \"thread_id\": \"123\",\n",
    "    \"agent_logs\": [\n",
    "        {\n",
    "            \"agent_name\": \"supervisor\",\n",
    "            \"task\": None,\n",
    "            \"result\": None,\n",
    "            \"step\": 0,\n",
    "            \"start_time\": None,\n",
    "            \"end_time\": None,\n",
    "            \"duration\": None,\n",
    "        }\n",
    "    ],\n",
    "    \"next_agent\": None,\n",
    "    \"prev_agent\": None,\n",
    "    \"task\": \"\",\n",
    "}\n",
    "config = {\"configurable\": {\"thread_id\": \"123\"}}\n",
    "async for data in workflow.astream(\n",
    "            input_state,\n",
    "            config=config,\n",
    "            stream_mode=[\"messages\"],\n",
    "        ):\n",
    "            data_type, (msg, meta_date) = data\n",
    "            if data_type == \"messages\":\n",
    "                print(msg.content)\n",
    "# (\n",
    "#     AIMessageChunk(\n",
    "#         content=\"ệt vời! Bạn muốn tôi tả một dòng sông như thế nào? Để tôi viết\",\n",
    "#         additional_kwargs={},\n",
    "#         response_metadata={\"safety_ratings\": []},\n",
    "#         id=\"run--e71ebc3d-bbba-4737-aea3-61c1e273329c\",\n",
    "#         usage_metadata={\n",
    "#             \"input_tokens\": 0,\n",
    "#             \"output_tokens\": 0,\n",
    "#             \"total_tokens\": 0,\n",
    "#             \"input_token_details\": {\"cache_read\": 0},\n",
    "#         },\n",
    "#     ),\n",
    "#     {\n",
    "#         \"thread_id\": \"123\",\n",
    "#         \"langgraph_step\": 1,\n",
    "#         \"langgraph_node\": \"writer\",\n",
    "#         \"langgraph_triggers\": (\"branch:to:writer\",),\n",
    "#         \"langgraph_path\": (\"__pregel_pull\", \"writer\"),\n",
    "#         \"langgraph_checkpoint_ns\": \"writer:0dbf21d6-2ddb-bf8e-fd2d-28e162e0965f|writer:01b812c0-3585-a730-9ff1-cba8f76a0c99\",\n",
    "#         \"checkpoint_ns\": \"writer:0dbf21d6-2ddb-bf8e-fd2d-28e162e0965f\",\n",
    "#         \"ls_provider\": \"google_genai\",\n",
    "#         \"ls_model_name\": \"gemini-2.0-flash\",\n",
    "#         \"ls_model_type\": \"chat\",\n",
    "#         \"ls_temperature\": 0.7,\n",
    "#     },\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47ebf41b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e:\\Project\\chatbot\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash\"\n",
      "  }\n",
      "  quota_value: 200\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 25\n",
      "}\n",
      "].\n"
     ]
    },
    {
     "ename": "ResourceExhausted",
     "evalue": "429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.0-flash\"\n  }\n  quota_value: 200\n}\n, links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n, retry_delay {\n  seconds: 23\n}\n]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mResourceExhausted\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 48\u001b[39m\n\u001b[32m     28\u001b[39m input_state = {\n\u001b[32m     29\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m     30\u001b[39m         HumanMessage(content=\u001b[33m\"\u001b[39m\u001b[33mđạo hàm là gì và ví dụ phân tích từng bước (bài toán thực tế)\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m   (...)\u001b[39m\u001b[32m     45\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtask\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     46\u001b[39m }\n\u001b[32m     47\u001b[39m config = {\u001b[33m\"\u001b[39m\u001b[33mconfigurable\u001b[39m\u001b[33m\"\u001b[39m: {\u001b[33m\"\u001b[39m\u001b[33mthread_id\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m123\u001b[39m\u001b[33m\"\u001b[39m}}\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m graph.astream(\n\u001b[32m     49\u001b[39m     input_state,\n\u001b[32m     50\u001b[39m     config=config,\n\u001b[32m     51\u001b[39m     stream_mode=[\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mupdates\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     52\u001b[39m ):\n\u001b[32m     53\u001b[39m     data_type, payload = event\n\u001b[32m     54\u001b[39m     \u001b[38;5;28mprint\u001b[39m(event)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Project\\chatbot\\.venv\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:2655\u001b[39m, in \u001b[36mPregel.astream\u001b[39m\u001b[34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, subgraphs)\u001b[39m\n\u001b[32m   2653\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m loop.amatch_cached_writes():\n\u001b[32m   2654\u001b[39m     loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2655\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner.atick(\n\u001b[32m   2656\u001b[39m     [t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m loop.tasks.values() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t.writes],\n\u001b[32m   2657\u001b[39m     timeout=\u001b[38;5;28mself\u001b[39m.step_timeout,\n\u001b[32m   2658\u001b[39m     get_waiter=get_waiter,\n\u001b[32m   2659\u001b[39m     schedule_task=loop.aaccept_push,\n\u001b[32m   2660\u001b[39m ):\n\u001b[32m   2661\u001b[39m     \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[32m   2662\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m output():\n\u001b[32m   2663\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m o\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Project\\chatbot\\.venv\\Lib\\site-packages\\langgraph\\pregel\\runner.py:401\u001b[39m, in \u001b[36mPregelRunner.atick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;66;03m# panic on failure or timeout\u001b[39;00m\n\u001b[32m    400\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m     \u001b[43m_panic_or_proceed\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdone\u001b[49m\u001b[43m.\u001b[49m\u001b[43munion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout_exc_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43masyncio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTimeoutError\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpanic\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    405\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    406\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    407\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m tb := exc.__traceback__:\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Project\\chatbot\\.venv\\Lib\\site-packages\\langgraph\\pregel\\runner.py:510\u001b[39m, in \u001b[36m_panic_or_proceed\u001b[39m\u001b[34m(futs, timeout_exc_cls, panic)\u001b[39m\n\u001b[32m    508\u001b[39m                 interrupts.append(exc)\n\u001b[32m    509\u001b[39m             \u001b[38;5;28;01melif\u001b[39;00m fut \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m SKIP_RERAISE_SET:\n\u001b[32m--> \u001b[39m\u001b[32m510\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m    511\u001b[39m \u001b[38;5;66;03m# raise combined interrupts\u001b[39;00m\n\u001b[32m    512\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m interrupts:\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Project\\chatbot\\.venv\\Lib\\site-packages\\langgraph\\pregel\\retry.py:136\u001b[39m, in \u001b[36marun_with_retry\u001b[39m\u001b[34m(task, retry_policies, stream, match_cached_writes, configurable)\u001b[39m\n\u001b[32m    134\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    135\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m task.proc.ainvoke(task.input, config)\n\u001b[32m    137\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    138\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Project\\chatbot\\.venv\\Lib\\site-packages\\langgraph\\utils\\runnable.py:672\u001b[39m, in \u001b[36mRunnableSeq.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    670\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    671\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m672\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.create_task(\n\u001b[32m    673\u001b[39m             step.ainvoke(\u001b[38;5;28minput\u001b[39m, config, **kwargs), context=context\n\u001b[32m    674\u001b[39m         )\n\u001b[32m    675\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    676\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[38;5;28;01mawait\u001b[39;00m step.ainvoke(\u001b[38;5;28minput\u001b[39m, config, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Project\\chatbot\\.venv\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:2788\u001b[39m, in \u001b[36mPregel.ainvoke\u001b[39m\u001b[34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, **kwargs)\u001b[39m\n\u001b[32m   2785\u001b[39m chunks: \u001b[38;5;28mlist\u001b[39m[Union[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any], Any]] = []\n\u001b[32m   2786\u001b[39m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] = []\n\u001b[32m-> \u001b[39m\u001b[32m2788\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.astream(\n\u001b[32m   2789\u001b[39m     \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   2790\u001b[39m     config,\n\u001b[32m   2791\u001b[39m     stream_mode=stream_mode,\n\u001b[32m   2792\u001b[39m     output_keys=output_keys,\n\u001b[32m   2793\u001b[39m     interrupt_before=interrupt_before,\n\u001b[32m   2794\u001b[39m     interrupt_after=interrupt_after,\n\u001b[32m   2795\u001b[39m     checkpoint_during=checkpoint_during,\n\u001b[32m   2796\u001b[39m     debug=debug,\n\u001b[32m   2797\u001b[39m     **kwargs,\n\u001b[32m   2798\u001b[39m ):\n\u001b[32m   2799\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode == \u001b[33m\"\u001b[39m\u001b[33mvalues\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   2800\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2801\u001b[39m             \u001b[38;5;28misinstance\u001b[39m(chunk, \u001b[38;5;28mdict\u001b[39m)\n\u001b[32m   2802\u001b[39m             \u001b[38;5;129;01mand\u001b[39;00m (ints := chunk.get(INTERRUPT)) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2803\u001b[39m         ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Project\\chatbot\\.venv\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:2655\u001b[39m, in \u001b[36mPregel.astream\u001b[39m\u001b[34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, subgraphs)\u001b[39m\n\u001b[32m   2653\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m loop.amatch_cached_writes():\n\u001b[32m   2654\u001b[39m     loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2655\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner.atick(\n\u001b[32m   2656\u001b[39m     [t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m loop.tasks.values() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t.writes],\n\u001b[32m   2657\u001b[39m     timeout=\u001b[38;5;28mself\u001b[39m.step_timeout,\n\u001b[32m   2658\u001b[39m     get_waiter=get_waiter,\n\u001b[32m   2659\u001b[39m     schedule_task=loop.aaccept_push,\n\u001b[32m   2660\u001b[39m ):\n\u001b[32m   2661\u001b[39m     \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[32m   2662\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m output():\n\u001b[32m   2663\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m o\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Project\\chatbot\\.venv\\Lib\\site-packages\\langgraph\\pregel\\runner.py:295\u001b[39m, in \u001b[36mPregelRunner.atick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[39m\n\u001b[32m    293\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    294\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m295\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m arun_with_retry(\n\u001b[32m    296\u001b[39m         t,\n\u001b[32m    297\u001b[39m         retry_policy,\n\u001b[32m    298\u001b[39m         stream=\u001b[38;5;28mself\u001b[39m.use_astream,\n\u001b[32m    299\u001b[39m         configurable={\n\u001b[32m    300\u001b[39m             CONFIG_KEY_CALL: partial(\n\u001b[32m    301\u001b[39m                 _acall,\n\u001b[32m    302\u001b[39m                 weakref.ref(t),\n\u001b[32m    303\u001b[39m                 stream=\u001b[38;5;28mself\u001b[39m.use_astream,\n\u001b[32m    304\u001b[39m                 retry=retry_policy,\n\u001b[32m    305\u001b[39m                 futures=weakref.ref(futures),\n\u001b[32m    306\u001b[39m                 schedule_task=schedule_task,\n\u001b[32m    307\u001b[39m                 submit=\u001b[38;5;28mself\u001b[39m.submit,\n\u001b[32m    308\u001b[39m                 loop=loop,\n\u001b[32m    309\u001b[39m             ),\n\u001b[32m    310\u001b[39m         },\n\u001b[32m    311\u001b[39m     )\n\u001b[32m    312\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    313\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Project\\chatbot\\.venv\\Lib\\site-packages\\langgraph\\pregel\\retry.py:136\u001b[39m, in \u001b[36marun_with_retry\u001b[39m\u001b[34m(task, retry_policies, stream, match_cached_writes, configurable)\u001b[39m\n\u001b[32m    134\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    135\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m task.proc.ainvoke(task.input, config)\n\u001b[32m    137\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    138\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Project\\chatbot\\.venv\\Lib\\site-packages\\langgraph\\utils\\runnable.py:672\u001b[39m, in \u001b[36mRunnableSeq.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    670\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    671\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m672\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.create_task(\n\u001b[32m    673\u001b[39m             step.ainvoke(\u001b[38;5;28minput\u001b[39m, config, **kwargs), context=context\n\u001b[32m    674\u001b[39m         )\n\u001b[32m    675\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    676\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[38;5;28;01mawait\u001b[39;00m step.ainvoke(\u001b[38;5;28minput\u001b[39m, config, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Project\\chatbot\\.venv\\Lib\\site-packages\\langgraph\\utils\\runnable.py:440\u001b[39m, in \u001b[36mRunnableCallable.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    438\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m run_manager.on_chain_end(ret)\n\u001b[32m    439\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m440\u001b[39m     ret = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.afunc(*args, **kwargs)\n\u001b[32m    441\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[32m    442\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m ret.ainvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Project\\chatbot\\src\\agents\\assigner\\assigner.py:27\u001b[39m, in \u001b[36mAssignerAgent.process\u001b[39m\u001b[34m(self, state)\u001b[39m\n\u001b[32m     25\u001b[39m message = state.get(\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m)[-\u001b[32m1\u001b[39m]\n\u001b[32m     26\u001b[39m start_time = time()\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._chain.ainvoke({\u001b[33m\"\u001b[39m\u001b[33massignment\u001b[39m\u001b[33m\"\u001b[39m: state[\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m]})\n\u001b[32m     28\u001b[39m end_time = time()\n\u001b[32m     29\u001b[39m state.update(\n\u001b[32m     30\u001b[39m     agent_logs=state.get(\u001b[33m\"\u001b[39m\u001b[33magent_logs\u001b[39m\u001b[33m\"\u001b[39m, [])\n\u001b[32m     31\u001b[39m     + [\n\u001b[32m   (...)\u001b[39m\u001b[32m     44\u001b[39m     human=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     45\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Project\\chatbot\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3089\u001b[39m, in \u001b[36mRunnableSequence.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3087\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3088\u001b[39m                 part = functools.partial(step.ainvoke, input_, config)\n\u001b[32m-> \u001b[39m\u001b[32m3089\u001b[39m             input_ = \u001b[38;5;28;01mawait\u001b[39;00m coro_with_context(part(), context, create_task=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m   3090\u001b[39m     \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[32m   3091\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Project\\chatbot\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:5444\u001b[39m, in \u001b[36mRunnableBindingBase.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   5437\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   5438\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mainvoke\u001b[39m(\n\u001b[32m   5439\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   5442\u001b[39m     **kwargs: Optional[Any],\n\u001b[32m   5443\u001b[39m ) -> Output:\n\u001b[32m-> \u001b[39m\u001b[32m5444\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.bound.ainvoke(\n\u001b[32m   5445\u001b[39m         \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   5446\u001b[39m         \u001b[38;5;28mself\u001b[39m._merge_configs(config),\n\u001b[32m   5447\u001b[39m         **{**\u001b[38;5;28mself\u001b[39m.kwargs, **kwargs},\n\u001b[32m   5448\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Project\\chatbot\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:394\u001b[39m, in \u001b[36mBaseChatModel.ainvoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    384\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    385\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mainvoke\u001b[39m(\n\u001b[32m    386\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    391\u001b[39m     **kwargs: Any,\n\u001b[32m    392\u001b[39m ) -> BaseMessage:\n\u001b[32m    393\u001b[39m     config = ensure_config(config)\n\u001b[32m--> \u001b[39m\u001b[32m394\u001b[39m     llm_result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.agenerate_prompt(\n\u001b[32m    395\u001b[39m         [\u001b[38;5;28mself\u001b[39m._convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[32m    396\u001b[39m         stop=stop,\n\u001b[32m    397\u001b[39m         callbacks=config.get(\u001b[33m\"\u001b[39m\u001b[33mcallbacks\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    398\u001b[39m         tags=config.get(\u001b[33m\"\u001b[39m\u001b[33mtags\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    399\u001b[39m         metadata=config.get(\u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    400\u001b[39m         run_name=config.get(\u001b[33m\"\u001b[39m\u001b[33mrun_name\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    401\u001b[39m         run_id=config.pop(\u001b[33m\"\u001b[39m\u001b[33mrun_id\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[32m    402\u001b[39m         **kwargs,\n\u001b[32m    403\u001b[39m     )\n\u001b[32m    404\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m, llm_result.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m]).message\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Project\\chatbot\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:968\u001b[39m, in \u001b[36mBaseChatModel.agenerate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m    959\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    960\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34magenerate_prompt\u001b[39m(\n\u001b[32m    961\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    965\u001b[39m     **kwargs: Any,\n\u001b[32m    966\u001b[39m ) -> LLMResult:\n\u001b[32m    967\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m--> \u001b[39m\u001b[32m968\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.agenerate(\n\u001b[32m    969\u001b[39m         prompt_messages, stop=stop, callbacks=callbacks, **kwargs\n\u001b[32m    970\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Project\\chatbot\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:926\u001b[39m, in \u001b[36mBaseChatModel.agenerate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    913\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[32m    914\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(\n\u001b[32m    915\u001b[39m             *[\n\u001b[32m    916\u001b[39m                 run_manager.on_llm_end(\n\u001b[32m   (...)\u001b[39m\u001b[32m    924\u001b[39m             ]\n\u001b[32m    925\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m926\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions[\u001b[32m0\u001b[39m]\n\u001b[32m    927\u001b[39m flattened_outputs = [\n\u001b[32m    928\u001b[39m     LLMResult(generations=[res.generations], llm_output=res.llm_output)  \u001b[38;5;66;03m# type: ignore[list-item, union-attr]\u001b[39;00m\n\u001b[32m    929\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[32m    930\u001b[39m ]\n\u001b[32m    931\u001b[39m llm_output = \u001b[38;5;28mself\u001b[39m._combine_llm_outputs([res.llm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Project\\chatbot\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1083\u001b[39m, in \u001b[36mBaseChatModel._agenerate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1077\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._should_stream(\n\u001b[32m   1078\u001b[39m     async_api=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m   1079\u001b[39m     run_manager=run_manager,\n\u001b[32m   1080\u001b[39m     **kwargs,\n\u001b[32m   1081\u001b[39m ):\n\u001b[32m   1082\u001b[39m     chunks: \u001b[38;5;28mlist\u001b[39m[ChatGenerationChunk] = []\n\u001b[32m-> \u001b[39m\u001b[32m1083\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._astream(messages, stop=stop, **kwargs):\n\u001b[32m   1084\u001b[39m         chunk.message.response_metadata = _gen_info_and_msg_metadata(chunk)\n\u001b[32m   1085\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_manager:\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Project\\chatbot\\.venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:1504\u001b[39m, in \u001b[36mChatGoogleGenerativeAI._astream\u001b[39m\u001b[34m(self, messages, stop, run_manager, tools, functions, safety_settings, tool_config, generation_config, cached_content, tool_choice, **kwargs)\u001b[39m\n\u001b[32m   1492\u001b[39m request = \u001b[38;5;28mself\u001b[39m._prepare_request(\n\u001b[32m   1493\u001b[39m     messages,\n\u001b[32m   1494\u001b[39m     stop=stop,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1501\u001b[39m     tool_choice=tool_choice,\n\u001b[32m   1502\u001b[39m )\n\u001b[32m   1503\u001b[39m prev_usage_metadata: UsageMetadata | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1504\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m _achat_with_retry(\n\u001b[32m   1505\u001b[39m     request=request,\n\u001b[32m   1506\u001b[39m     generation_method=\u001b[38;5;28mself\u001b[39m.async_client.stream_generate_content,\n\u001b[32m   1507\u001b[39m     **kwargs,\n\u001b[32m   1508\u001b[39m     metadata=\u001b[38;5;28mself\u001b[39m.default_metadata,\n\u001b[32m   1509\u001b[39m ):\n\u001b[32m   1510\u001b[39m     _chat_result = _response_to_result(\n\u001b[32m   1511\u001b[39m         chunk, stream=\u001b[38;5;28;01mTrue\u001b[39;00m, prev_usage=prev_usage_metadata\n\u001b[32m   1512\u001b[39m     )\n\u001b[32m   1513\u001b[39m     gen = cast(ChatGenerationChunk, _chat_result.generations[\u001b[32m0\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Project\\chatbot\\.venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:243\u001b[39m, in \u001b[36m_achat_with_retry\u001b[39m\u001b[34m(generation_method, **kwargs)\u001b[39m\n\u001b[32m    240\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    241\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m--> \u001b[39m\u001b[32m243\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m _achat_with_retry(**kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Project\\chatbot\\.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:189\u001b[39m, in \u001b[36mAsyncRetrying.wraps.<locals>.async_wrapped\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    187\u001b[39m copy = \u001b[38;5;28mself\u001b[39m.copy()\n\u001b[32m    188\u001b[39m async_wrapped.statistics = copy.statistics  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m189\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m copy(fn, *args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Project\\chatbot\\.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:111\u001b[39m, in \u001b[36mAsyncRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    109\u001b[39m retry_state = RetryCallState(retry_object=\u001b[38;5;28mself\u001b[39m, fn=fn, args=args, kwargs=kwargs)\n\u001b[32m    110\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m     do = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter(retry_state=retry_state)\n\u001b[32m    112\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    113\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Project\\chatbot\\.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:153\u001b[39m, in \u001b[36mAsyncRetrying.iter\u001b[39m\u001b[34m(self, retry_state)\u001b[39m\n\u001b[32m    151\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.actions:\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m action(retry_state)\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Project\\chatbot\\.venv\\Lib\\site-packages\\tenacity\\_utils.py:99\u001b[39m, in \u001b[36mwrap_to_async_func.<locals>.inner\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     98\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minner\u001b[39m(*args: typing.Any, **kwargs: typing.Any) -> typing.Any:\n\u001b[32m---> \u001b[39m\u001b[32m99\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Project\\chatbot\\.venv\\Lib\\site-packages\\tenacity\\__init__.py:418\u001b[39m, in \u001b[36mBaseRetrying._post_stop_check_actions.<locals>.exc_check\u001b[39m\u001b[34m(rs)\u001b[39m\n\u001b[32m    416\u001b[39m retry_exc = \u001b[38;5;28mself\u001b[39m.retry_error_cls(fut)\n\u001b[32m    417\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.reraise:\n\u001b[32m--> \u001b[39m\u001b[32m418\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mretry_exc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    419\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m retry_exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfut\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexception\u001b[39;00m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Project\\chatbot\\.venv\\Lib\\site-packages\\tenacity\\__init__.py:185\u001b[39m, in \u001b[36mRetryError.reraise\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    183\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mreraise\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> t.NoReturn:\n\u001b[32m    184\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.last_attempt.failed:\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlast_attempt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    186\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Project\\chatbot\\.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:114\u001b[39m, in \u001b[36mAsyncRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    112\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    113\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m         result = \u001b[38;5;28;01mawait\u001b[39;00m fn(*args, **kwargs)\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[32m    116\u001b[39m         retry_state.set_exception(sys.exc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Project\\chatbot\\.venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:241\u001b[39m, in \u001b[36m_achat_with_retry.<locals>._achat_with_retry\u001b[39m\u001b[34m(**kwargs)\u001b[39m\n\u001b[32m    237\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ChatGoogleGenerativeAIError(\n\u001b[32m    238\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid argument provided to Gemini: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    239\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    240\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m241\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Project\\chatbot\\.venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:234\u001b[39m, in \u001b[36m_achat_with_retry.<locals>._achat_with_retry\u001b[39m\u001b[34m(**kwargs)\u001b[39m\n\u001b[32m    231\u001b[39m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[32m    232\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_achat_with_retry\u001b[39m(**kwargs: Any) -> Any:\n\u001b[32m    233\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m234\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m generation_method(**kwargs)\n\u001b[32m    235\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m InvalidArgument \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    236\u001b[39m         \u001b[38;5;66;03m# Do not retry for these errors.\u001b[39;00m\n\u001b[32m    237\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m ChatGoogleGenerativeAIError(\n\u001b[32m    238\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid argument provided to Gemini: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    239\u001b[39m         ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Project\\chatbot\\.venv\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary_async.py:231\u001b[39m, in \u001b[36mAsyncRetry.__call__.<locals>.retry_wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    227\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"A wrapper that calls target function with retry.\"\"\"\u001b[39;00m\n\u001b[32m    228\u001b[39m sleep_generator = exponential_sleep_generator(\n\u001b[32m    229\u001b[39m     \u001b[38;5;28mself\u001b[39m._initial, \u001b[38;5;28mself\u001b[39m._maximum, multiplier=\u001b[38;5;28mself\u001b[39m._multiplier\n\u001b[32m    230\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m231\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m retry_target(\n\u001b[32m    232\u001b[39m     functools.partial(func, *args, **kwargs),\n\u001b[32m    233\u001b[39m     predicate=\u001b[38;5;28mself\u001b[39m._predicate,\n\u001b[32m    234\u001b[39m     sleep_generator=sleep_generator,\n\u001b[32m    235\u001b[39m     timeout=\u001b[38;5;28mself\u001b[39m._timeout,\n\u001b[32m    236\u001b[39m     on_error=on_error,\n\u001b[32m    237\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Project\\chatbot\\.venv\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary_async.py:163\u001b[39m, in \u001b[36mretry_target\u001b[39m\u001b[34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[39m\n\u001b[32m    159\u001b[39m \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[32m    160\u001b[39m \u001b[38;5;66;03m# This function explicitly must deal with broad exceptions.\u001b[39;00m\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    162\u001b[39m     \u001b[38;5;66;03m# defer to shared logic for handling errors\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m163\u001b[39m     next_sleep = \u001b[43m_retry_error_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdeadline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[43m        \u001b[49m\u001b[43msleep_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m        \u001b[49m\u001b[43merror_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpredicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m        \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexception_factory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    173\u001b[39m     \u001b[38;5;66;03m# if exception not raised, sleep before next attempt\u001b[39;00m\n\u001b[32m    174\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m asyncio.sleep(next_sleep)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Project\\chatbot\\.venv\\Lib\\site-packages\\google\\api_core\\retry\\retry_base.py:214\u001b[39m, in \u001b[36m_retry_error_helper\u001b[39m\u001b[34m(exc, deadline, sleep_iterator, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[39m\n\u001b[32m    208\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m predicate_fn(exc):\n\u001b[32m    209\u001b[39m     final_exc, source_exc = exc_factory_fn(\n\u001b[32m    210\u001b[39m         error_list,\n\u001b[32m    211\u001b[39m         RetryFailureReason.NON_RETRYABLE_ERROR,\n\u001b[32m    212\u001b[39m         original_timeout,\n\u001b[32m    213\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m214\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m final_exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msource_exc\u001b[39;00m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m on_error_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    216\u001b[39m     on_error_fn(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Project\\chatbot\\.venv\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary_async.py:158\u001b[39m, in \u001b[36mretry_target\u001b[39m\u001b[34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[39m\n\u001b[32m    156\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    157\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m target()\n\u001b[32m    159\u001b[39m     \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[32m    160\u001b[39m     \u001b[38;5;66;03m# This function explicitly must deal with broad exceptions.\u001b[39;00m\n\u001b[32m    161\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    162\u001b[39m         \u001b[38;5;66;03m# defer to shared logic for handling errors\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Project\\chatbot\\.venv\\Lib\\site-packages\\google\\api_core\\grpc_helpers_async.py:178\u001b[39m, in \u001b[36m_wrap_stream_errors.<locals>.error_remapped_callable\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    176\u001b[39m call = callable_(*args, **kwargs)\n\u001b[32m    177\u001b[39m call = wrapper_type().with_call(call)\n\u001b[32m--> \u001b[39m\u001b[32m178\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m call.wait_for_connection()\n\u001b[32m    179\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m call\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Project\\chatbot\\.venv\\Lib\\site-packages\\google\\api_core\\grpc_helpers_async.py:79\u001b[39m, in \u001b[36m_WrappedCall.wait_for_connection\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     77\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call.wait_for_connection()\n\u001b[32m     78\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m grpc.RpcError \u001b[38;5;28;01mas\u001b[39;00m rpc_error:\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions.from_grpc_error(rpc_error) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrpc_error\u001b[39;00m\n",
      "\u001b[31mResourceExhausted\u001b[39m: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.0-flash\"\n  }\n  quota_value: 200\n}\n, links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n, retry_delay {\n  seconds: 23\n}\n]",
      "During task with name 'assigner' and id '1f22eec3-dfaf-5904-7d7d-e5a4ce2a9934'",
      "During task with name 'assigner' and id '340801df-b8da-c732-3b80-3063fee2a72f'"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables.graph import MermaidDrawMethod\n",
    "from IPython.display import display, Image\n",
    "import sys, os\n",
    "from langchain_core.messages import HumanMessage\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\",\"..\")))\n",
    "print(os.path.abspath(os.path.join(os.getcwd(),\"..\",\"..\")))\n",
    "from src.agents.workflow import graph\n",
    "from src.agents.analyst.analyst import AnalystAgent\n",
    "from src.agents.writer.writer import WriterAgent\n",
    "from src.agents.assigner.assigner import AssignerAgent\n",
    "from src.agents.workflow import graph\n",
    "# display(\n",
    "#     Image(\n",
    "#         graph.get_graph().draw_mermaid_png(\n",
    "#             draw_method=MermaidDrawMethod.API,\n",
    "#         ),\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# graph1 = AssignerAgent()\n",
    "# display(\n",
    "#     Image(\n",
    "#         graph1.get_graph().get_graph().draw_mermaid_png(\n",
    "#             draw_method=MermaidDrawMethod.API,\n",
    "#         ),\n",
    "#     )\n",
    "# )\n",
    "input_state = {\n",
    "    \"messages\": [\n",
    "        HumanMessage(content=\"đạo hàm là gì và ví dụ phân tích từng bước (bài toán thực tế)\"),\n",
    "    ],\n",
    "    \"thread_id\": \"123\",\n",
    "    \"agent_logs\": [\n",
    "        {\n",
    "            \"agent_name\": \"supervisor\",\n",
    "            \"task\": None,\n",
    "            \"result\": None,\n",
    "            \"start_time\": None,\n",
    "            \"end_time\": None,\n",
    "            \"duration\": None,\n",
    "        }\n",
    "    ],\n",
    "    \"next_agent\": None,\n",
    "    \"prev_agent\": None,\n",
    "    \"task\": \"\",\n",
    "}\n",
    "config = {\"configurable\": {\"thread_id\": \"123\"}}\n",
    "async for event in graph.astream(\n",
    "    input_state,\n",
    "    config=config,\n",
    "    stream_mode=[\"messages\",\"updates\"],\n",
    "):\n",
    "    data_type, payload = event\n",
    "    print(event)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d8ec72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "supervisor (0.62s): assignment for writer\n",
      "writer (0.63s):\n",
      " , how can I help you today?\n"
     ]
    }
   ],
   "source": [
    "res = await workflow.ainvoke(\n",
    "    {\"messages\": [HumanMessage(content=\"hello\")]},\n",
    "    config={\"configurable\": {\"thread_id\": \"123\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b550a937",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='hello', additional_kwargs={}, response_metadata={}, id='39f8cd7c-ecdb-4728-8546-9f1213939de9'),\n",
       " AIMessage(content='Hey there! How can I help you today?', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--172a6047-251e-4d9d-a6b4-e492ba0b61e0-0', usage_metadata={'input_tokens': 27, 'output_tokens': 11, 'total_tokens': 38, 'input_token_details': {'cache_read': 0}})]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[\"messages\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf387ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "supervisor (0.68s): assignment for analysis\n",
      "analysis (0.75s): solved\n",
      "logic (2.79s): solved\n",
      "writer (3.70s): solved\n",
      "trả lời tiếng việt\n",
      "ví dụ 1 bài toán về machine learning và giải thích từng bước\n",
      "This is a great, comprehensive breakdown of the process for tackling a click-through rate (CTR) prediction problem! You've covered all the crucial steps, from defining the problem to monitoring the deployed model. The explanations are clear and the inclusion of specific techniques and examples within each step is very helpful.\n",
      "\n",
      "Here are a few of my thoughts and potential expansions, presented as natural follow-up questions or considerations:\n",
      "\n",
      "*   **Data Collection & Feature Understanding:** You mentioned gathering data on user demographics, browsing history, etc.  What are some potential challenges in collecting this data, and how might you address them (e.g., data privacy concerns, incomplete data, data silos)?  How would you prioritize which features to focus on initially? Would you perform any exploratory data analysis (EDA) to understand the relationships between features and the target variable *before* feature engineering?\n",
      "\n",
      "*   **Feature Engineering - Beyond the Basics:** You provided some good examples. What other, more advanced, feature engineering techniques might be considered for this problem? (e.g., using natural language processing (NLP) on ad copy or user search queries, creating interaction features between different variables, or using external data sources).\n",
      "\n",
      "*   **Model Selection - Considerations Beyond Accuracy:** You mentioned starting with a simpler model. Besides accuracy, what other factors might influence your model selection (e.g., interpretability, training time, deployment complexity, explainability for stakeholders)?  Are there specific business constraints that would favor one model over another?\n",
      "\n",
      "*   **Evaluation Metrics - Choosing the Right One:** You listed several evaluation metrics.  In a real-world CTR prediction scenario, which metric would likely be the most important to optimize for, and why?  (e.g., Is precision more important than recall if you want to avoid showing ads to users who are unlikely to click?)  How would you handle class imbalance (CTR is often very low)?\n",
      "\n",
      "*   **Deployment - Practical Considerations:** You mentioned deployment.  What are some common deployment architectures for a model like this? How would you handle the real-time prediction requirements? What are some tools and technologies commonly used for deploying machine learning models?\n",
      "\n",
      "*   **Monitoring - Detecting and Addressing Model Drift:** You mentioned monitoring performance. What are some key indicators of model drift (i.e., when the model's performance degrades over time)? How would you automatically detect and address model drift?\n",
      "\n",
      "Overall, this is an excellent overview. My questions above are just intended to prompt further thought and discussion on some of the more nuanced aspects of solving this type of machine learning problem.  What would you like to explore in more detail? Perhaps a deeper dive into feature engineering, model selection trade-offs, or deployment strategies?\n",
      "trả lời tiếng việt\n",
      "bài toán cụ thể hơn\n",
      "Lời giải của bạn gần đúng rồi, nhưng có một chút sai sót nhỏ trong bước kiểm tra. Sai số không phải do làm tròn, mà do kết quả giải phương trình có sai sót. Để mình giải lại chi tiết hơn nhé:\n",
      "\n",
      "**Giải:**\n",
      "\n",
      "1.  **Gọi ẩn:** Gọi quãng đường AB là x (km) (x > 0).\n",
      "2.  **Thời gian đi và về:**\n",
      "    *   Thời gian đi từ A đến B: x/12 (giờ)\n",
      "    *   Thời gian đi từ B về A: x/10 (giờ)\n",
      "3.  **Đổi đơn vị:** 30 phút = 0.5 giờ\n",
      "4.  **Lập phương trình:** Tổng thời gian cả đi lẫn về (bao gồm cả thời gian nghỉ) là 5 giờ. Vậy thời gian đi và về (không tính nghỉ) là: 5 - 0.5 = 4.5 (giờ). Ta có phương trình:\n",
      "\n",
      "    x/12 + x/10 = 4.5\n",
      "5.  **Giải phương trình:**\n",
      "    *   Quy đồng mẫu số: (5x + 6x) / 60 = 4.5\n",
      "    *   11x / 60 = 4.5\n",
      "    *   11x = 4.5 * 60\n",
      "    *   11x = 270\n",
      "    *   x = 270 / 11\n",
      "    *   x ≈ 24.55 (km)\n",
      "\n",
      "**Vậy, quãng đường AB là khoảng 24.55 km.**\n",
      "\n",
      "**Kiểm tra lại:**\n",
      "\n",
      "*   Thời gian đi: 24.55 km / 12 km/h ≈ 2.05 giờ\n",
      "*   Thời gian về: 24.55 km / 10 km/h ≈ 2.455 giờ\n",
      "*   Tổng thời gian (không tính nghỉ): 2.05 giờ + 2.455 giờ ≈ 4.505 giờ\n",
      "*   Tổng thời gian (tính nghỉ): 4.505 giờ + 0.5 giờ ≈ 5.005 giờ (sai số rất nhỏ do làm tròn)\n",
      "\n",
      "**Kết luận:**\n",
      "\n",
      "Quãng đường AB là khoảng **24.55 km**.  Kết quả ban đầu của bạn là 24 km chưa chính xác lắm.\n"
     ]
    }
   ],
   "source": [
    "res = await workflow.ainvoke(\n",
    "    {\"messages\": [SystemMessage(content=\"trả lời tiếng việt\"),HumanMessage(content=\"bài toán cụ thể hơn\")]},\n",
    "    config={\"configurable\": {\"thread_id\": \"123\"}},\n",
    ")\n",
    "\n",
    "for msg in res[\"messages\"]:\n",
    "    print(msg.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8adc1ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((), 'updates', {'node_1': {'foo': 'hi! foo'}})\n",
      "(('node_2:0af2105d-237e-4926-c55a-4e58383281e9',), 'updates', {'subgraph_node_1': {'bar': 'bar'}})\n",
      "(('node_2:0af2105d-237e-4926-c55a-4e58383281e9',), 'updates', {'subgraph_node_2': {'foo': 'hi! foobar'}})\n",
      "((), 'updates', {'node_2': {'foo': 'hi! foobar'}})\n"
     ]
    }
   ],
   "source": [
    "from langgraph.graph import START, StateGraph\n",
    "from typing import TypedDict\n",
    "\n",
    "\n",
    "# Define subgraph\n",
    "class SubgraphState(TypedDict):\n",
    "    foo: str  # note that this key is shared with the parent graph state\n",
    "    bar: str\n",
    "\n",
    "\n",
    "def subgraph_node_1(state: SubgraphState):\n",
    "    return {\"bar\": \"bar\"}\n",
    "\n",
    "\n",
    "def subgraph_node_2(state: SubgraphState):\n",
    "    return {\"foo\": state[\"foo\"] + state[\"bar\"]}\n",
    "\n",
    "\n",
    "subgraph_builder = StateGraph(SubgraphState)\n",
    "subgraph_builder.add_node(subgraph_node_1)\n",
    "subgraph_builder.add_node(subgraph_node_2)\n",
    "subgraph_builder.add_edge(START, \"subgraph_node_1\")\n",
    "subgraph_builder.add_edge(\"subgraph_node_1\", \"subgraph_node_2\")\n",
    "subgraph = subgraph_builder.compile()\n",
    "\n",
    "\n",
    "# Define parent graph\n",
    "class ParentState(TypedDict):\n",
    "    foo: str\n",
    "\n",
    "\n",
    "def node_1(state: ParentState):\n",
    "    return {\"foo\": \"hi! \" + state[\"foo\"]}\n",
    "\n",
    "\n",
    "builder = StateGraph(ParentState)\n",
    "builder.add_node(\"node_1\", node_1)\n",
    "builder.add_node(\"node_2\", subgraph)\n",
    "builder.add_edge(START, \"node_1\")\n",
    "builder.add_edge(\"node_1\", \"node_2\")\n",
    "graph = builder.compile()\n",
    "\n",
    "for chunk in graph.stream(\n",
    "    {\"foo\": \"foo\"},\n",
    "    stream_mode=[\"updates\",\"messages\"],\n",
    "    subgraphs=True,\n",
    "):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb427aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Run until interrupt ===\n",
      "ok\n",
      "Event: {'__interrupt__': (Interrupt(value={'text_to_revise': 'original text'}, resumable=True, ns=['human_node:79a99f9c-1a07-3b20-29fb-af9742ac7e9e']),)}\n",
      "State values: StateSnapshot(values={'some_text': 'original text'}, next=('human_node',), config={'configurable': {'thread_id': '3018111c-dfdd-48d4-a986-d53efb564270', 'checkpoint_ns': '', 'checkpoint_id': '1f09210c-0308-6c83-8000-4e0b235be942'}}, metadata={'source': 'loop', 'writes': None, 'step': 0, 'parents': {}, 'thread_id': '3018111c-dfdd-48d4-a986-d53efb564270'}, created_at='2025-09-15T08:48:29.002457+00:00', parent_config={'configurable': {'thread_id': '3018111c-dfdd-48d4-a986-d53efb564270', 'checkpoint_ns': '', 'checkpoint_id': '1f09210c-0306-655d-bfff-91e5de33288c'}}, tasks=(PregelTask(id='79a99f9c-1a07-3b20-29fb-af9742ac7e9e', name='human_node', path=('__pregel_pull', 'human_node'), error=None, interrupts=(Interrupt(value={'text_to_revise': 'original text'}, resumable=True, ns=['human_node:79a99f9c-1a07-3b20-29fb-af9742ac7e9e']),), state=None, result=None),), interrupts=(Interrupt(value={'text_to_revise': 'original text'}, resumable=True, ns=['human_node:79a99f9c-1a07-3b20-29fb-af9742ac7e9e']),))\n",
      "Interrupt: None\n",
      "=== Resume with edited text ===\n",
      "ok\n",
      "ok1\n",
      "Event: {'human_node': {'some_text': 'Edited text'}}\n"
     ]
    }
   ],
   "source": [
    "from typing import TypedDict\n",
    "import uuid\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langgraph.constants import START\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.types import interrupt, Command\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    some_text: str\n",
    "\n",
    "\n",
    "def human_node(state: State):\n",
    "    print(\"ok\")\n",
    "    value = interrupt({\"text_to_revise\": state[\"some_text\"]})\n",
    "    print(\"ok1\")\n",
    "    return {\"some_text\": value}\n",
    "\n",
    "\n",
    "# Build the graph\n",
    "graph_builder = StateGraph(State)\n",
    "graph_builder.add_node(\"human_node\", human_node)\n",
    "graph_builder.add_edge(START, \"human_node\")\n",
    "checkpointer = InMemorySaver()\n",
    "graph = graph_builder.compile(checkpointer=checkpointer)\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": str(uuid.uuid4())}}\n",
    "\n",
    "# --- Run until interrupt ---\n",
    "print(\"=== Run until interrupt ===\")\n",
    "for event in graph.stream({\"some_text\": \"original text\"}, config=config):\n",
    "    print(\"Event:\", event)\n",
    "\n",
    "# Lấy state khi bị interrupt\n",
    "snapshot = graph.get_state(config)\n",
    "print(\"State values:\", snapshot)\n",
    "print(\"Interrupt:\", snapshot.metadata.get(\"__interrupt__\"))\n",
    "\n",
    "# --- Resume ---\n",
    "print(\"=== Resume with edited text ===\")\n",
    "for event in graph.stream(Command(resume=\"Edited text\"), config=config):\n",
    "    print(\"Event:\", event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1781150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e:\\Project\\chatbot\n",
      "Error loading supported_apps.json: [Errno 2] No such file or directory: 'src/config/system/supported_apps.json'\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_core.runnables.graph import MermaidDrawMethod\n",
    "from IPython.display import display, Image\n",
    "import sys, os\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\", \"..\")))\n",
    "print(os.path.abspath(os.path.join(os.getcwd(), \"..\", \"..\")))\n",
    "from src.agents.workflow import graph\n",
    "from src.agents.analyst.analyst import AnalystAgent\n",
    "from src.agents.writer.writer import WriterAgent\n",
    "from src.agents.assigner.assigner import AssignerAgent\n",
    "from src.agents.workflow import graph\n",
    "\n",
    "display(\n",
    "    Image(\n",
    "        graph.get_graph().draw_mermaid_png(\n",
    "            draw_method=MermaidDrawMethod.API,\n",
    "        ),\n",
    "    )\n",
    ")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
